{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#import dask.dataframe as dd\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import cv2\n",
    "import os\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "root_path = Path('/home/ec2-user/SageMaker/defect_detection/')#.resolve()\n",
    "\n",
    "code_path = root_path / \"notebooks/WM-811K/src/\"\n",
    "code_path.mkdir(exist_ok=True)\n",
    "data_path = root_path / \"data/MIR-WM811K/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Creating SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Create the execution script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/data_augmentation/program/augmentation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/data_augmentation/program/augmentation.py\n",
    "import numpy as np\n",
    "import tarfile\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--limit\", type=int, default=None)\n",
    "    parser.add_argument(\"--augmented-size\", type=int, default=2000)\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def load_models(model_path: str):\n",
    "    # Add tar decompression here\n",
    "    model_tar = Path(model_path) / \"model.tar.gz\"\n",
    "    tf_file = tarfile.open(str(model_tar), mode=\"r:gz\")\n",
    "    dest_dir = Path(\"/tmp/models\")\n",
    "    dest_dir.mkdir()\n",
    "    tf_file.extractall(path=str(dest_dir))\n",
    "    tf_file.close()\n",
    "    encoder = load_model(Path(dest_dir) / \"encoder.h5\")\n",
    "    decoder = load_model(Path(dest_dir) / \"decoder.h5\")\n",
    "    return encoder, decoder\n",
    "\n",
    "\n",
    "def load_data(path: str, file_name: str=\"data.npz\", limit: int=None):\n",
    "    file_path = Path(path) / file_name\n",
    "    with np.load(str(file_path), allow_pickle=True) as data:\n",
    "        x = data['x']\n",
    "        y = data['y']\n",
    "        label_classes = data['label_classes'].item(0)\n",
    "    if limit:\n",
    "        return (x[:limit], y[:limit], label_classes)\n",
    "    else:\n",
    "        return (x, y, label_classes)\n",
    "    \n",
    "    \n",
    "def generate_augmented_data(wafers, label, encoder, decoder, n_examples):\n",
    "    # Encode input wafer\n",
    "    logging.info(f\"There are {len(wafers)} examples for {label}\")\n",
    "    encoded_x = encoder.predict(wafers)\n",
    "    \n",
    "    additional_example_batches = n_examples // wafer.shape[0] + 1\n",
    "    temp_noised = []\n",
    "    for i in range(additional_example_batches):\n",
    "        temp_noised.append(encoded_x + np.random.normal(loc=0, scale=0.1, size = (len(encoded_x), 13, 13, 64)))\n",
    "    noised_encoded_x = np.vstack(temp_noised)\n",
    "    gen_x = decoder.predict(noised_encoded_x[1:])\n",
    "    # also make label vector with same length\n",
    "    gen_y = np.full((len(gen_x), ), label)\n",
    "\n",
    "    logging.info(f\"Returning {n_examples - len(wafer)} new examples\") \n",
    "    return gen_x[1:n_examples+1], gen_y[1:n_examples+1]\n",
    "\n",
    "\n",
    "def augment(x, y, labels, encoder, decoder, n_examples):\n",
    "    aug_x = x.copy()\n",
    "    aug_y = y.copy()\n",
    "    for l in labels: \n",
    "        # skip none case\n",
    "        if l in {'none', 'unknown'} : \n",
    "            continue\n",
    "        else:\n",
    "            logging.info(f'Generating {l}')\n",
    "\n",
    "        gen_x, gen_y = generate_augmented_data(x[np.where(y==l)[0]], l, encoder, decoder, n_examples)\n",
    "        aug_x = np.concatenate((aug_x, gen_x), axis=0)\n",
    "        aug_y = np.concatenate((aug_y, gen_y))\n",
    "    return aug_x, aug_y\n",
    "\n",
    "\n",
    "def save_augmented(x, y, output_path):\n",
    "    np.savez_compressed(output_path / \"data.npz\", x=x, y=y)\n",
    "\n",
    "                        \n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_arguments()\n",
    "    root_path = Path('/opt/ml/processing')\n",
    "    model_path = root_path / \"models\"\n",
    "    data_path = root_path / \"data\"\n",
    "    augmented_path = root_path / \"augmented\"\n",
    "    x, y, label_classes = load_data(str(data_path), limit=args.limit)\n",
    "    encoder, decoder = load_models(str(model_path))\n",
    "    x, y = generate_augmentation(x, y, list(label_classes.keys()), encoder, decoder, args.augmented_size)\n",
    "    save_augmented(x, y, augmented_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Build a Container for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "ecr_repository = 'data-augmentation'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/SageMaker/defect_detection/notebooks/WM-811K/src/data_augmentation ~/SageMaker/defect_detection/notebooks/WM-811K\n",
      "Sending build context to Docker daemon  13.82kB\n",
      "Step 1/15 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-gpu-py3\n",
      " ---> 43a74e93a483\n",
      "Step 2/15 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 0e0a4fe719b7\n",
      "Step 3/15 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil ffmpeg libsm6 libxext6\n",
      " ---> Using cache\n",
      " ---> bd665a670110\n",
      "Step 4/15 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> a12df01f221b\n",
      "Step 5/15 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> de8e4caee4ab\n",
      "Step 6/15 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 7ab1fdfd2da8\n",
      "Step 7/15 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 0644a25711e6\n",
      "Step 8/15 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> ddc2111be8d3\n",
      "Step 9/15 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> 3768ac45e7fa\n",
      "Step 10/15 : RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64\n",
      " ---> Using cache\n",
      " ---> 6d628513ec65\n",
      "Step 11/15 : RUN chmod +x /usr/local/bin/dumb-init\n",
      " ---> Using cache\n",
      " ---> d6a387afb28e\n",
      "Step 12/15 : RUN mkdir /opt/app\n",
      " ---> Using cache\n",
      " ---> 194e3b2c8bdf\n",
      "Step 13/15 : COPY program /opt/program\n",
      " ---> 0edf83d2fb7b\n",
      "Step 14/15 : RUN chmod +x /opt/program/augmentation.py\n",
      " ---> Running in 905a7c4c7358\n",
      "Removing intermediate container 905a7c4c7358\n",
      " ---> f47037f17b7d\n",
      "Step 15/15 : ENTRYPOINT [\"python3\"]\n",
      " ---> Running in fa99c107008f\n",
      "Removing intermediate container fa99c107008f\n",
      " ---> c9b41af8b6af\n",
      "Successfully built c9b41af8b6af\n",
      "Successfully tagged data-augmentation:latest\n",
      "~/SageMaker/defect_detection/notebooks/WM-811K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: AWS CLI version 2, the latest major version of the AWS CLI, is now stable and recommended for general use. For more information, see the AWS CLI version 2 installation instructions at: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\n",
      "\n",
      "usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]\n",
      "To see help text, you can run:\n",
      "\n",
      "  aws help\n",
      "  aws <command> help\n",
      "  aws <command> <subcommand> help\n",
      "aws: error: argument --region: expected one argument\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "pushd src/data_augmentation\n",
    "$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "docker build -t  data-augmentation .\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'data-augmentation' already exists in the registry with id '160951647621'\n",
      "The push refers to repository [160951647621.dkr.ecr.us-east-1.amazonaws.com/data-augmentation]\n",
      "\n",
      "\u001b[1B48bf6f13: Preparing \n",
      "\u001b[1B4c23698f: Preparing \n",
      "\u001b[1Bb8edb1eb: Preparing \n",
      "\u001b[1Bd3858bc6: Preparing \n",
      "\u001b[1Bbd70a43c: Preparing \n",
      "\u001b[1Bb267bb8a: Preparing \n",
      "\u001b[1B7470f0dd: Preparing \n",
      "\u001b[1Bec6e212a: Preparing \n",
      "\u001b[1Bf319a508: Preparing \n",
      "\u001b[1Bf7132110: Preparing \n",
      "\u001b[1Beb9ebda6: Preparing \n",
      "\u001b[7Bb267bb8a: Waiting g \n",
      "\u001b[7B7470f0dd: Waiting g \n",
      "\u001b[7Bec6e212a: Waiting g \n",
      "\u001b[1B62cacce5: Preparing \n",
      "\u001b[1Bd22b16ab: Preparing \n",
      "\u001b[1B26dec4ac: Preparing \n",
      "\u001b[1B6ff78197: Preparing \n",
      "\u001b[1Bdf5cf960: Preparing \n",
      "\u001b[7B5bf23a91: Waiting g \n",
      "\u001b[1Bb763c8de: Preparing \n",
      "\u001b[13B7132110: Waiting g \n",
      "\u001b[8Bd22b16ab: Waiting g \n",
      "\u001b[14Bb9ebda6: Waiting g \n",
      "\u001b[9B26dec4ac: Waiting g \n",
      "\u001b[15B829d3bc: Waiting g \n",
      "\u001b[15B4b15037: Waiting g \n",
      "\u001b[1Ba4b22186: Preparing \n",
      "\u001b[12Bff78197: Waiting g \n",
      "\u001b[12Bf5cf960: Waiting g \n",
      "\u001b[12Bfe6a1c2: Waiting g \n",
      "\u001b[32B8bf6f13: Pushed lready exists 9kB\u001b[32A\u001b[2K\u001b[20A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:8c237e06054b66f9d59004bd041ffeee8c39206db9b42a107de7bb9c7e909058 size: 7011\n"
     ]
    }
   ],
   "source": [
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $repository_uri\n",
    "!docker push $repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import IPython\n",
    "# dist_version = '2.9.2.dev0'\n",
    "# !aws s3 cp s3://gianpo-public/sagemaker-{dist_version}.tar.gz .\n",
    "# !{sys.executable} -m pip install -q -U pip\n",
    "# !{sys.executable} -m pip install -q sagemaker-{dist_version}.tar.gz\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "data_augmenter = ScriptProcessor(\n",
    "    base_job_name=\"data-augmentation\",\n",
    "    image_uri=repository_uri,\n",
    "    command=[\"python3\"],\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"local\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  data-augmentation-2020-10-01-23-12-45-656\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-01-16-22-10-257/model', 'LocalPath': '/opt/ml/processing/models', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-09-30-22-55-17-588/output/autoencoder/train', 'LocalPath': '/opt/ml/processing/data', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/data-augmentation-2020-10-01-23-12-45-656/input/code/augmentation.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'classifier/train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/data-augmentation-2020-10-01-23-12-45-656/output/classifier/train', 'LocalPath': '/opt/ml/processing/augmented', 'S3UploadMode': 'EndOfJob'}}]\n",
      "Creating q0asqc01l9-algo-1-vs1gd ... \n",
      "\u001b[1BAttaching to q0asqc01l9-algo-1-vs1gd2mdone\u001b[0m\n",
      "\u001b[36mq0asqc01l9-algo-1-vs1gd |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[36mq0asqc01l9-algo-1-vs1gd |\u001b[0m   File \"/opt/ml/processing/input/code/augmentation.py\", line 85, in <module>\n",
      "\u001b[36mq0asqc01l9-algo-1-vs1gd |\u001b[0m     x, y, label_classes = load_data(str(data_path), limit=args.limit)\n",
      "\u001b[36mq0asqc01l9-algo-1-vs1gd |\u001b[0m   File \"/opt/ml/processing/input/code/augmentation.py\", line 32, in load_data\n",
      "\u001b[36mq0asqc01l9-algo-1-vs1gd |\u001b[0m     x = data['x']\n",
      "\u001b[36mq0asqc01l9-algo-1-vs1gd |\u001b[0m   File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 266, in __getitem__\n",
      "\u001b[36mq0asqc01l9-algo-1-vs1gd |\u001b[0m     raise KeyError(\"%s is not a file in the archive\" % key)\n",
      "\u001b[36mq0asqc01l9-algo-1-vs1gd |\u001b[0m KeyError: 'x is not a file in the archive'\n",
      "\u001b[36mq0asqc01l9-algo-1-vs1gd exited with code 1\n",
      "\u001b[0mAborting on container exit...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run: ['docker-compose', '-f', '/tmp/tmpuedjwwrl/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, processing_inputs, processing_output_config, environment, processing_job_name)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0m_stream_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36m_stream_output\u001b[0;34m(process)\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexit_code\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Process exited with code: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mexit_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Process exited with code: 1",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-71a910c54697>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         )\n\u001b[1;32m     12\u001b[0m     ],\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mProcessingOutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'classifier/train'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/opt/ml/processing/augmented'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, code, inputs, outputs, arguments, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalized_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormalized_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m             \u001b[0mexperiment_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m         )\n\u001b[1;32m    462\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/processing.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, processor, inputs, outputs, experiment_config)\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m         \u001b[0;31m# Call sagemaker_session.process using the arguments dictionary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m         \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mprocess_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         return cls(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, inputs, output_config, job_name, resources, stopping_condition, app_specification, environment, network_config, role_arn, tags, experiment_config)\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating processing-job with name %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_processing_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mprocess_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m     def _get_process_request(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/local_session.py\u001b[0m in \u001b[0;36mcreate_processing_job\u001b[0;34m(self, ProcessingJobName, AppSpecification, ProcessingResources, Environment, ProcessingInputs, ProcessingOutputConfig, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting processing job\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         processing_job.start(ProcessingInputs, ProcessingOutputConfig, Environment, \n\u001b[0;32m--> 115\u001b[0;31m             ProcessingJobName)\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mLocalSagemakerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_processing_jobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mProcessingJobName\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessing_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/entities.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, processing_inputs, processing_output_config, environment, processing_job_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m         self.container.process(\n\u001b[1;32m    120\u001b[0m             \u001b[0mprocessing_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_output_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 processing_job_name)\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_COMPLETED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/sagemaker/local/image.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, processing_inputs, processing_output_config, environment, processing_job_name)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# which contains the exit code and append the command line to it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Failed to run: %s, %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcompose_command\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0;31m# Uploading processing outputs back to Amazon S3.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to run: ['docker-compose', '-f', '/tmp/tmpuedjwwrl/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1"
     ]
    }
   ],
   "source": [
    "data_augmenter.run(\n",
    "    code=\"src/data_augmentation/program/augmentation.py\",\n",
    "    arguments=[\"--limit\", \"100\", \"--augmented-size\", \"15000\"],\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=\"s3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-01-16-22-10-257/model\",\n",
    "            destination='/opt/ml/processing/models'\n",
    "        ), ProcessingInput(\n",
    "            source=\"s3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-09-30-22-55-17-588/output/autoencoder/train\",\n",
    "            destination=\"/opt/ml/processing/data\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name='classifier/train', source='/opt/ml/processing/augmented')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "processed_data = data_augmenter.latest_job.describe()['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bucket, *path = processed_data.split(\"/\")[2:]\n",
    "path = \"/\".join(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sagemaker.utils.download_file(bucket, path + \"/data.npz\", \"/tmp/data.npz\", sagemaker.session.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with np.load(\"/tmp/data.npz\", allow_pickle=True) as data:\n",
    "    x = data['arr_0']\n",
    "    y = data['arr_1']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
