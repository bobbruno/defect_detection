{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "import sys\n",
    "import IPython\n",
    "dist_version = '2.9.2.dev0'\n",
    "!aws s3 cp s3://gianpo-public/sagemaker-{dist_version}.tar.gz .\n",
    "!{sys.executable} -m pip install -q -U pip\n",
    "!{sys.executable} -m pip install -q sagemaker-{dist_version}.tar.gz\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The cell above is for access to a new experimental feature for faster debugging of processing jobs. It will be released on the SageMaker SDK following the standard release process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This notebook runs a processing job for generating additional wafer maps using the encode -> add noise -> decode strategy we built before. It has a very similar structure to the data processing notebook, since the general steps for building and running processing are similar. We built a separate docker image to have access to Tensorflow. We're using the AWS Optimized [Deep Learning Containers](https://docs.aws.amazon.com/deep-learning-containers/latest/devguide/deep-learning-containers-images.html) Tensorflow image as the base for our processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "root_path = Path('/home/ec2-user/SageMaker/defect_detection/')#.resolve()\n",
    "\n",
    "code_path = root_path / \"notebooks/WM-811K/src/\"\n",
    "code_path.mkdir(exist_ok=True)\n",
    "data_path = root_path / \"data/MIR-WM811K/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "ecr_repository = 'data-augmentation'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'160951647621.dkr.ecr.us-east-1.amazonaws.com/data-augmentation:latest'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Create the execution script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtarfile\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mmodels\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m load_model\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_arguments\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--limit\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--augmented-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m2000\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_models\u001b[39;49;00m(model_path: \u001b[36mstr\u001b[39;49;00m):\n",
      "    \u001b[37m# Add tar decompression here\u001b[39;49;00m\n",
      "    model_tar = Path(model_path) / \u001b[33m\"\u001b[39;49;00m\u001b[33mmodel.tar.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mLoading models from \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mmodel_tar\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    tf_file = tarfile.open(\u001b[36mstr\u001b[39;49;00m(model_tar), mode=\u001b[33m\"\u001b[39;49;00m\u001b[33mr:gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    dest_dir = Path(\u001b[33m\"\u001b[39;49;00m\u001b[33m/tmp/models\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    dest_dir.mkdir()\n",
      "    tf_file.extractall(path=\u001b[36mstr\u001b[39;49;00m(dest_dir))\n",
      "    tf_file.close()\n",
      "    encoder = load_model(Path(dest_dir) / \u001b[33m\"\u001b[39;49;00m\u001b[33mencoder.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mEncoder loaded\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    decoder = load_model(Path(dest_dir) / \u001b[33m\"\u001b[39;49;00m\u001b[33mdecoder.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33mDecoder loaded\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mreturn\u001b[39;49;00m encoder, decoder\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_data\u001b[39;49;00m(path: \u001b[36mstr\u001b[39;49;00m, file_name: \u001b[36mstr\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33mdata.npz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, limit: \u001b[36mint\u001b[39;49;00m=\u001b[34mNone\u001b[39;49;00m):\n",
      "    file_path = Path(path) / file_name\n",
      "    \u001b[34mwith\u001b[39;49;00m np.load(\u001b[36mstr\u001b[39;49;00m(file_path), allow_pickle=\u001b[34mTrue\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m data:\n",
      "        x = data[\u001b[33m'\u001b[39;49;00m\u001b[33mx\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        y = data[\u001b[33m'\u001b[39;49;00m\u001b[33my\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        label_classes = data[\u001b[33m'\u001b[39;49;00m\u001b[33mlabel_classes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m].item(\u001b[34m0\u001b[39;49;00m)\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mx shape: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mx.shape\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33my shape: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mx.shape\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \u001b[34mif\u001b[39;49;00m limit:\n",
      "        \u001b[34mreturn\u001b[39;49;00m (x[:limit], y[:limit], label_classes)\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mreturn\u001b[39;49;00m (x, y, label_classes)\n",
      "    \n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mgenerate_augmented_data\u001b[39;49;00m(wafers, label, encoder, decoder, n_examples):\n",
      "    \u001b[37m# Encode input wafer\u001b[39;49;00m\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mThere are \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[36mlen\u001b[39;49;00m(wafers)\u001b[33m}\u001b[39;49;00m\u001b[33m examples for \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mlabel\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    encoded_x = encoder.predict(wafers)\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mShape of encoded_x: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mencoded_x.shape\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    \n",
      "    additional_example_batches = n_examples // wafer.shape[\u001b[34m0\u001b[39;49;00m] + \u001b[34m1\u001b[39;49;00m\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mGenerating \u001b[39;49;00m\u001b[33m{\u001b[39;49;00madditional_example_batches\u001b[33m}\u001b[39;49;00m\u001b[33m batches of examples for label \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mlabel\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    temp_noised = []\n",
      "    \u001b[34mfor\u001b[39;49;00m i \u001b[35min\u001b[39;49;00m \u001b[36mrange\u001b[39;49;00m(additional_example_batches):\n",
      "        temp_noised.append(encoded_x + np.random.normal(loc=\u001b[34m0\u001b[39;49;00m, scale=\u001b[34m0.1\u001b[39;49;00m, size = (\u001b[36mlen\u001b[39;49;00m(encoded_x), \u001b[34m13\u001b[39;49;00m, \u001b[34m13\u001b[39;49;00m, \u001b[34m64\u001b[39;49;00m)))\n",
      "    noised_encoded_x = np.vstack(temp_noised)\n",
      "    gen_x = decoder.predict(noised_encoded_x)\n",
      "    \u001b[37m# also make label vector with same length\u001b[39;49;00m\n",
      "    gen_y = np.full((\u001b[36mlen\u001b[39;49;00m(gen_x), ), label)\n",
      "\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mReturning \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mn_examples - \u001b[36mlen\u001b[39;49;00m(wafer)\u001b[33m}\u001b[39;49;00m\u001b[33m new examples\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \n",
      "    \u001b[34mreturn\u001b[39;49;00m gen_x, gen_y\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32maugment\u001b[39;49;00m(x, y, labels, encoder, decoder, n_examples):\n",
      "    aug_x = x.copy()\n",
      "    aug_y = y.copy()\n",
      "    \u001b[34mfor\u001b[39;49;00m l \u001b[35min\u001b[39;49;00m labels: \n",
      "        \u001b[37m# skip none case\u001b[39;49;00m\n",
      "        \u001b[34mif\u001b[39;49;00m l \u001b[35min\u001b[39;49;00m {\u001b[33m'\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33munknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m} : \n",
      "            \u001b[34mcontinue\u001b[39;49;00m\n",
      "        \u001b[34melse\u001b[39;49;00m:\n",
      "            logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mGenerating \u001b[39;49;00m\u001b[33m{\u001b[39;49;00ml\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "        gen_x, gen_y = generate_augmented_data(x[np.where(y==l)[\u001b[34m0\u001b[39;49;00m]], l, encoder, decoder, n_examples)\n",
      "        aug_x = np.concatenate((aug_x, gen_x), axis=\u001b[34m0\u001b[39;49;00m)\n",
      "        aug_y = np.concatenate((aug_y, gen_y))\n",
      "    \u001b[34mreturn\u001b[39;49;00m aug_x, aug_y\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32msave_augmented\u001b[39;49;00m(x, y, output_path):\n",
      "    np.savez_compressed(output_path / \u001b[33m\"\u001b[39;49;00m\u001b[33mdata.npz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, x=x, y=y)\n",
      "\n",
      "                        \n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    args = parse_arguments()\n",
      "    logging.info(\u001b[33m\"\u001b[39;49;00m\u001b[33margs: \u001b[39;49;00m\u001b[33m{args}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    root_path = Path(\u001b[33m'\u001b[39;49;00m\u001b[33m/opt/ml/processing\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    model_path = root_path / \u001b[33m\"\u001b[39;49;00m\u001b[33mmodels\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    data_path = root_path / \u001b[33m\"\u001b[39;49;00m\u001b[33mdata\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    augmented_path = root_path / \u001b[33m\"\u001b[39;49;00m\u001b[33maugmented\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\n",
      "    x, y, label_classes = load_data(\u001b[36mstr\u001b[39;49;00m(data_path), limit=args.limit)\n",
      "    encoder, decoder = load_models(\u001b[36mstr\u001b[39;49;00m(model_path))\n",
      "    x, y = augment(x, y, \u001b[36mlist\u001b[39;49;00m(label_classes.keys()), encoder, decoder, args.augmented_size)\n",
      "    save_augmented(x, y, augmented_path)\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./src/data_augmentation/program/augmentation.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Build a Container for augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/SageMaker/defect_detection/notebooks/WM-811K/src/data_augmentation ~/SageMaker/defect_detection/notebooks/WM-811K\n",
      "Sending build context to Docker daemon  13.82kB\n",
      "Step 1/15 : FROM 763104351884.dkr.ecr.us-east-1.amazonaws.com/tensorflow-training:2.1.0-gpu-py3\n",
      " ---> 43a74e93a483\n",
      "Step 2/15 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> 0e0a4fe719b7\n",
      "Step 3/15 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil ffmpeg libsm6 libxext6\n",
      " ---> Using cache\n",
      " ---> bd665a670110\n",
      "Step 4/15 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> a12df01f221b\n",
      "Step 5/15 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> de8e4caee4ab\n",
      "Step 6/15 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 7ab1fdfd2da8\n",
      "Step 7/15 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 0644a25711e6\n",
      "Step 8/15 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> ddc2111be8d3\n",
      "Step 9/15 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> 3768ac45e7fa\n",
      "Step 10/15 : RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64\n",
      " ---> Using cache\n",
      " ---> 6d628513ec65\n",
      "Step 11/15 : RUN chmod +x /usr/local/bin/dumb-init\n",
      " ---> Using cache\n",
      " ---> d6a387afb28e\n",
      "Step 12/15 : RUN mkdir /opt/app\n",
      " ---> Using cache\n",
      " ---> 194e3b2c8bdf\n",
      "Step 13/15 : COPY program /opt/program\n",
      " ---> 0edf83d2fb7b\n",
      "Step 14/15 : RUN chmod +x /opt/program/augmentation.py\n",
      " ---> Running in 905a7c4c7358\n",
      "Removing intermediate container 905a7c4c7358\n",
      " ---> f47037f17b7d\n",
      "Step 15/15 : ENTRYPOINT [\"python3\"]\n",
      " ---> Running in fa99c107008f\n",
      "Removing intermediate container fa99c107008f\n",
      " ---> c9b41af8b6af\n",
      "Successfully built c9b41af8b6af\n",
      "Successfully tagged data-augmentation:latest\n",
      "~/SageMaker/defect_detection/notebooks/WM-811K\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: AWS CLI version 2, the latest major version of the AWS CLI, is now stable and recommended for general use. For more information, see the AWS CLI version 2 installation instructions at: https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html\n",
      "\n",
      "usage: aws [options] <command> <subcommand> [<subcommand> ...] [parameters]\n",
      "To see help text, you can run:\n",
      "\n",
      "  aws help\n",
      "  aws <command> help\n",
      "  aws <command> <subcommand> help\n",
      "aws: error: argument --region: expected one argument\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "pushd src/data_augmentation\n",
    "$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "docker build -t  data-augmentation .\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "Collapsed": "false",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'data-augmentation' already exists in the registry with id '160951647621'\n",
      "The push refers to repository [160951647621.dkr.ecr.us-east-1.amazonaws.com/data-augmentation]\n",
      "\n",
      "\u001b[1B48bf6f13: Preparing \n",
      "\u001b[1B4c23698f: Preparing \n",
      "\u001b[1Bb8edb1eb: Preparing \n",
      "\u001b[1Bd3858bc6: Preparing \n",
      "\u001b[1Bbd70a43c: Preparing \n",
      "\u001b[1Bb267bb8a: Preparing \n",
      "\u001b[1B7470f0dd: Preparing \n",
      "\u001b[1Bec6e212a: Preparing \n",
      "\u001b[1Bf319a508: Preparing \n",
      "\u001b[1Bf7132110: Preparing \n",
      "\u001b[1Beb9ebda6: Preparing \n",
      "\u001b[7Bb267bb8a: Waiting g \n",
      "\u001b[7B7470f0dd: Waiting g \n",
      "\u001b[7Bec6e212a: Waiting g \n",
      "\u001b[1B62cacce5: Preparing \n",
      "\u001b[1Bd22b16ab: Preparing \n",
      "\u001b[1B26dec4ac: Preparing \n",
      "\u001b[1B6ff78197: Preparing \n",
      "\u001b[1Bdf5cf960: Preparing \n",
      "\u001b[7B5bf23a91: Waiting g \n",
      "\u001b[1Bb763c8de: Preparing \n",
      "\u001b[13B7132110: Waiting g \n",
      "\u001b[8Bd22b16ab: Waiting g \n",
      "\u001b[14Bb9ebda6: Waiting g \n",
      "\u001b[9B26dec4ac: Waiting g \n",
      "\u001b[15B829d3bc: Waiting g \n",
      "\u001b[15B4b15037: Waiting g \n",
      "\u001b[1Ba4b22186: Preparing \n",
      "\u001b[12Bff78197: Waiting g \n",
      "\u001b[12Bf5cf960: Waiting g \n",
      "\u001b[12Bfe6a1c2: Waiting g \n",
      "\u001b[32B8bf6f13: Pushed lready exists 9kB\u001b[32A\u001b[2K\u001b[20A\u001b[2K\u001b[17A\u001b[2K\u001b[13A\u001b[2K\u001b[10A\u001b[2K\u001b[5A\u001b[2K\u001b[1A\u001b[2Klatest: digest: sha256:8c237e06054b66f9d59004bd041ffeee8c39206db9b42a107de7bb9c7e909058 size: 7011\n"
     ]
    }
   ],
   "source": [
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $repository_uri\n",
    "!docker push $repository_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Run the Container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "data_augmenter = ScriptProcessor(\n",
    "    base_job_name=\"data-augmentation\",\n",
    "    image_uri=repository_uri,\n",
    "    command=[\"python3\"],\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"local\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  data-augmentation-2020-10-05-21-59-11-375\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/output', 'LocalPath': '/opt/ml/processing/models', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'input-2', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-10-05-21-26-16-330/output/autoencoder/train', 'LocalPath': '/opt/ml/processing/data', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/data-augmentation-2020-10-05-21-59-11-375/input/code/augmentation.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'classifier/train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/data-augmentation-2020-10-05-21-59-11-375/output/classifier/train', 'LocalPath': '/opt/ml/processing/augmented', 'S3UploadMode': 'EndOfJob'}}]\n",
      "Creating u1p5edc98o-algo-1-epce6 ... \n",
      "\u001b[1BAttaching to u1p5edc98o-algo-1-epce62mdone\u001b[0m\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:x shape: (22894, 26, 26, 3)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:y shape: (22894,)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Loading models from /opt/ml/processing/models/model.tar.gz\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m 2020-10-05 21:59:18.911018: W tensorflow/stream_executor/platform/default/dso_loader.cc:55] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/extras/CUPTI/lib64:/usr/local/openmpi/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m 2020-10-05 21:59:18.911061: E tensorflow/stream_executor/cuda/cuda_driver.cc:351] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m 2020-10-05 21:59:18.911107: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:163] no NVIDIA GPU device is present: /dev/nvidia0 does not exist\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m 2020-10-05 21:59:18.940502: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2300000000 Hz\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m 2020-10-05 21:59:18.941230: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x59469a0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m 2020-10-05 21:59:18.941263: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m Model: \"model_1\"\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m _________________________________________________________________\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m Layer (type)                 Output Shape              Param #   \n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m =================================================================\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m input_1 (InputLayer)         [(None, 26, 26, 3)]       0         \n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m _________________________________________________________________\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m conv2d (Conv2D)              (None, 26, 26, 64)        1792      \n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m _________________________________________________________________\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m max_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m =================================================================\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m Total params: 1,792\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m Trainable params: 1,792\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m Non-trainable params: 0\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m _________________________________________________________________\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Encoder loaded: None\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m Model: \"model_2\"\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m _________________________________________________________________\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m Layer (type)                 Output Shape              Param #   \n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m =================================================================\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m input_2 (InputLayer)         [(None, 13, 13, 64)]      0         \n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m _________________________________________________________________\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m conv2d_transpose (Conv2DTran (None, 13, 13, 64)        36928     \n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m _________________________________________________________________\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m up_sampling2d (UpSampling2D) (None, 26, 26, 64)        0         \n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m _________________________________________________________________\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m conv2d_transpose_1 (Conv2DTr (None, 26, 26, 3)         1731      \n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m =================================================================\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m Total params: 38,659\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m Trainable params: 38,659\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m Non-trainable params: 0\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m _________________________________________________________________\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Decoder loaded: None\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating Center\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:There are 803 examples for Center\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Shape of encoded_x: (803, 13, 13, 64)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating 3 batches of examples for label Center\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Returning 1197 new examples\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating Edge-Loc\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:There are 680 examples for Edge-Loc\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Shape of encoded_x: (680, 13, 13, 64)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating 3 batches of examples for label Edge-Loc\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Returning 1320 new examples\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating Edge-Ring\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:There are 1093 examples for Edge-Ring\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Shape of encoded_x: (1093, 13, 13, 64)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating 2 batches of examples for label Edge-Ring\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Returning 907 new examples\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating Loc\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:There are 634 examples for Loc\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Shape of encoded_x: (634, 13, 13, 64)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating 4 batches of examples for label Loc\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Returning 1366 new examples\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating Near-full\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:There are 32 examples for Near-full\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Shape of encoded_x: (32, 13, 13, 64)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating 63 batches of examples for label Near-full\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Returning 1968 new examples\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating Random\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:There are 282 examples for Random\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Shape of encoded_x: (282, 13, 13, 64)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating 8 batches of examples for label Random\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Returning 1718 new examples\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating Scratch\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:There are 165 examples for Scratch\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Shape of encoded_x: (165, 13, 13, 64)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating 13 batches of examples for label Scratch\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Returning 1835 new examples\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating Donut\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:There are 14 examples for Donut\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Shape of encoded_x: (14, 13, 13, 64)\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Generating 143 batches of examples for label Donut\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 |\u001b[0m INFO:root:Returning 1986 new examples\n",
      "\u001b[36mu1p5edc98o-algo-1-epce6 exited with code 0\n",
      "\u001b[0mAborting on container exit...\n",
      "===== Job Complete =====\n",
      "."
     ]
    }
   ],
   "source": [
    "data_augmenter.run(\n",
    "    code=\"src/data_augmentation/program/augmentation.py\",\n",
    "    arguments=[\"--augmented-size\", \"2000\"],\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=\"s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/output\",\n",
    "            destination='/opt/ml/processing/models'\n",
    "        ), ProcessingInput(\n",
    "            source=\"s3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-10-05-21-26-16-330/output/autoencoder/train\",\n",
    "            destination=\"/opt/ml/processing/data\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[ProcessingOutput(output_name='classifier/train', source='/opt/ml/processing/augmented')]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We can now check the results of the processing job. The `latest_job` property holds all information about what happened on it, including where the generated file is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "processed_data = data_augmenter.latest_job.describe()['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bucket, *path = processed_data.split(\"/\")[2:]\n",
    "path = \"/\".join(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output was saved to bucket sagemaker-us-east-1-160951647621, under the folder data-augmentation-2020-10-05-21-59-11-375/output/classifier/train\n"
     ]
    }
   ],
   "source": [
    "bucket, *path = processed_data.split(\"/\")[2:]\n",
    "path = \"/\".join(path)\n",
    "print(f\"The output was saved to bucket {bucket}, under the folder {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Download the file and check the resulting vectors inside it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sagemaker.utils.download_file(bucket, path + \"/data.npz\", \"/tmp/data.npz\", sagemaker.session.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with np.load(\"/tmp/data.npz\", allow_pickle=True) as data:\n",
    "    x = data['x']\n",
    "    y = data['y']\n",
    "    label_classes = data['label_classes'].item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (40484, 26, 26, 3)\n",
      "Y shape: (40484,)\n",
      "Labels: {'Center': 0, 'Edge-Loc': 1, 'Edge-Ring': 2, 'Loc': 3, 'Near-full': 4, 'Random': 5, 'Scratch': 6, 'none': 7, 'Donut': 8}\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape: {x.shape}\\nY shape: {y.shape}\\nLabels: {label_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "After augmentation, we almost doubled the total number of examples. This new dataset will be used to [train the classifier model](train_classifier.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
