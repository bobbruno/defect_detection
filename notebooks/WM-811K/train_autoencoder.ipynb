{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Running on Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Input, models\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22894 samples\n",
      "Epoch 1/15\n",
      "22894/22894 - 21s - loss: 0.1218\n",
      "Epoch 2/15\n",
      "22894/22894 - 0s - loss: 0.0737\n",
      "Epoch 3/15\n",
      "22894/22894 - 0s - loss: 0.0638\n",
      "Epoch 4/15\n",
      "22894/22894 - 0s - loss: 0.0529\n",
      "Epoch 5/15\n",
      "22894/22894 - 0s - loss: 0.0435\n",
      "Epoch 6/15\n",
      "22894/22894 - 0s - loss: 0.0375\n",
      "Epoch 7/15\n",
      "22894/22894 - 0s - loss: 0.0320\n",
      "Epoch 8/15\n",
      "22894/22894 - 0s - loss: 0.0271\n",
      "Epoch 9/15\n",
      "22894/22894 - 0s - loss: 0.0231\n",
      "Epoch 10/15\n",
      "22894/22894 - 0s - loss: 0.0201\n",
      "Epoch 11/15\n",
      "22894/22894 - 0s - loss: 0.0178\n",
      "Epoch 12/15\n",
      "22894/22894 - 0s - loss: 0.0160\n",
      "Epoch 13/15\n",
      "22894/22894 - 0s - loss: 0.0145\n",
      "Epoch 14/15\n",
      "22894/22894 - 0s - loss: 0.0133\n",
      "Epoch 15/15\n",
      "22894/22894 - 0s - loss: 0.0122\n"
     ]
    }
   ],
   "source": [
    "def create_autoencoder(data, epochs, batch_size):\n",
    "    input_shape = (26, 26, 3)\n",
    "    input_tensor = Input(input_shape)\n",
    "    encode = layers.Conv2D(64, (3,3), padding='same', activation='relu')(input_tensor)\n",
    "\n",
    "    latent_vector = layers.MaxPool2D()(encode)\n",
    "\n",
    "    # Decoder\n",
    "    decode_layer_1 = layers.Conv2DTranspose(64, (3,3), padding='same', activation='relu')\n",
    "    decode_layer_2 = layers.UpSampling2D()\n",
    "    output_tensor = layers.Conv2DTranspose(3, (3,3), padding='same', activation='sigmoid')\n",
    "\n",
    "    # connect decoder layers\n",
    "    decode = decode_layer_1(latent_vector)\n",
    "    decode = decode_layer_2(decode)\n",
    "\n",
    "    ae = models.Model(input_tensor, output_tensor(decode))\n",
    "    ae.compile(\n",
    "        optimizer = 'Adam',\n",
    "        loss = 'mse',\n",
    "    )\n",
    "    ae.fit(data, data,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=2)\n",
    "    \n",
    "    decoder_input = Input((13, 13, 64))\n",
    "    decoder = decode_layer_1(decoder_input)\n",
    "    decoder = decode_layer_2(decoder)\n",
    "    \n",
    "    return ae, models.Model(input_tensor, latent_vector), models.Model(decoder_input, output_tensor(decoder))\n",
    "\n",
    "\n",
    "def load_data(bucket: str, path: str, file_name: str=\"data.npz\", temp_folder: str=\"/tmp\"):\n",
    "    file_path = Path(path) / file_name\n",
    "    temp_file = Path(temp_folder) / file_name\n",
    "    sagemaker.utils.download_file(bucket, str(file_path), str(temp_file), sagemaker.session.Session())\n",
    "    with np.load(str(temp_file), allow_pickle=True) as data:\n",
    "        x = data['x']\n",
    "        y = data['y']\n",
    "    return (x, y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    x, y = load_data(\n",
    "        \"sagemaker-us-east-1-160951647621\",\n",
    "        \"wafer-data-processing-2020-10-04-21-48-20-207/output/autoencoder/train\",\n",
    "    )\n",
    "    ae, encoder, decoder = create_autoencoder(x, epochs=15, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "encoded_x = encoder.predict(x)\n",
    "noised_encoded_x = encoded_x + np.random.normal(loc=0, scale=0.1, size = (len(encoded_x), 13, 13, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABLUAAAGICAYAAAC3GdLsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de5RkZ3kf6t9rjZC4GslcImEB9ghfiHOQk7GQg2MTK1iYhGA7CYNiY8ghlicOiaUlJyacZQNOwJcFFjlJDmMwBMx1QGDAJ8QCFCPCCRJIRBiwbDMDAoGEBAgsAeb+nT/2Hk+p1T3TXVNdVV/X86ylNd1Vtet76+vq/dN+69u7q7UWAAAAAOjJtyy6AAAAAADYKk0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7mlp0q6qeXlW/O+vHbuK5WlWdOYvnAmD1VNV/r6onHeX+l1bVf5hjPV+oqu+c13gArI6qemRVfWLRdbBzaWqxFKrqyVX1gar6UlV9qqpeUFX3Pto2rbXntNb++WaefyuPBaB/VXV9VX21qu6z5vZrxw8nHryYypLW2o+31l421vPkqnrXtM9VVQ8eX89/W3P7K6rqmZus5x6ttY9MWwPATjNmyF+OTf9PjR823GPRda2nqp5ZVa/Yxuef6wctsFWaWixcVV2c5DeT/Jsk35rknCQPSvK2qrrLBtvsml+FAHTqo0nOP/xNVf2NJHddXDnb6pyqesSiiwDYQR7bWrtHkrOSfH+Sf7fgeqZSA8f97Fje3CxUVd0rybOS/KvW2h+21r7WWrs+yeMzNLZ+ZnzcM6vq0vGT59uSPHntpxJV9bNV9bGq+mxV/cr4Ccvfm9j+FePXhz/VflJVfbyqPlNV/9fE85xdVe+uqs9X1U1V9Z83aq6t83reUVX/oar+1/jJzh9U1bdV1Sur6raqeu/k6oCq+o9VdcN43zVV9Xcm7jv8mg9U1e1V9b6qetjUkw2wel6e5Gcnvn9Skt+bfEBV/f2q+t/jfviGydVNE3lxQVXdOGbCxesNVFXfMebGt4zf/25V3TJx/yuq6sLx63dU1T+vqu9Nsj/JD46Z8fmJpzylqv7buP+/qqp2H+O1/laSDT9Jr6qfq6qDVXVrVb25qk6fuO+vTquvqsdU1Z+M436yqn5p4nH/YFzp9vkx5/6PY9QE0L3W2qeSXJahuZUkqaqTquq547HEzVW1v6ruOnH/48b95W1VdaiqHj3efvq4D7513Cf/3MQ2z6yq11bV74374A9V1Z6J+3953C/fXlV/VlXnjs/79CR7xxx5//jYd1TVs6vq/0vypSTfOXlsNDHe5LHUD4379s+PefjkqrogyU8n+beHj20mXsfrq+rTVfXRqvrXE89z1xpWd32uqv4kyQ8cbX7HDPqFqvrw+Nr+fVXtruF47LZxTu4yPvaUqvp/x3E/N3797RPP9Y6q+vWqek9V/UVVvamqTt3kj5pOaWqxaH87yclJ3jB5Y2vtC0n+e5JHTdz8uCSXJrl3kldOPr6qHprk/8mw0z0tw4qvBxxj7B9K8t1Jzk3yq+PBRZJ8I8lFSe6T5AfH+39hC6/pCUmeOI6/O8m7k/zXJKcmuS7JMyYe+94MAXlqklcleV1VnTxx/+OSvG7i/jdW1YlbqAVglV2Z5F5V9b1VdUKSvUnWnqLxxQyNr3sn+ftJ/kVV/cSax/zdJA9J8mNJnjZ5UHBYa+2jSW7L8Gl+kvydJF+YyJYfTnLFmm2uS7IvybvHUwAnT7s/P8OHPqckOZjk2cd4rf8lyXetV1tV/WiSX8/wgdFpST6W5DUbPM+Lk/x8a+2eSb4vyf8Yn+NvJnlJkp9P8m1JfifJm6vqpGPUBdC1sWny4xn2xYf9ZpLvyvD/8Wdm+P/+Xx0ff3aGD1D+TYZs+eEk14/bvTrJJ5KcnuQfJ3lOVZ078bz/MMP++d5J3pzkP4/P+d1JnprkB8b983lJrm+t/WGS5yQ5MObI5AfgT0xyQZJ7ZtjvH+01PjDDsdd/SnLf8XVd21p7YYbjrt8an/+x44c3f5Dk/ePrPjfJhVV13vh0z8hwDLR7rHPDa0hOeHSSv5XhjJ1/m+SFGY7rzsiQRYdXXX9LhuOqByV5YJK/PDxHE342yf+ZYY6/nuT/3sT4dExTi0W7T5LPtNa+vs59N433H/bu1tobW2vfbK395ZrH/uMkf9Bae1dr7asZQqUdY+xntdb+srX2/gw75YclSWvtmtbala21r4+rxn4nyY9s4TX919baodbaX2QIh0OttbePr/F1OXLAk9baK1prnx3Hel6SkzI02g67prV2aWvta0l+O0MD8Jwt1AKw6g6v1npUkj9N8snJO1tr72itfWDMlj/OcMCxdp//rNbaF1trH8jwP9PnZ31XJPmRqvpr4/eXjt9/R5J7ZciazXpDa+09Y3a8MhMrBDbw5QyNr/VWa/10kpe01t7XWvtKhlNofrDWv67Y15I8tKru1Vr7XGvtfePtP5fkd1prV7XWvjFeE+wrkUnAzvXGqro9yQ1Jbsn4wXRVVYZ94kWttVtba7dnaCw9YdzuKRn2uW8bs+WTrbU/raozMnyo/suttS+31q5N8rsZmk+Hvau19pbW2jcy5NfhJtU3MhwnPLSqTmytXd9aO3SM+l/aWvvQeJzxtWM89qeTvL219urxzJnPjvWt5weS3Le19mutta+O12R80cTrf3ySZ49zc0M211T6zdbaba21DyX5YJK3ttY+MnE89f1JMtb1+tbal8Z5f3bunNkvb619sLX2xSS/kuTx4wdb7FCaWizaZ5Lcp9a/RtZp4/2H3XCU5zl98v7W2peSfPYYY39q4usvJblHklTVd41LWT9Vw6mOz8kdm2vHcvPE13+5zvd/dZHJqrq4qq4bl8d+PsMKs8mxJl/TN3Pkkx0ANuflSf5pkidnzamHSVJVD6+qPxpPZfiLDCun1u7zJ/PnY9l4P3xFkkdm+FT+nUnekeF/tn8kyf8c9+ObtW5GHcOLkty/qh675vbTM/Ep/bga+rNZf0XzP0rymCQfq6orquoHx9sflOTi8bSUz4+ZdUZkErBz/cS4KuqRSb4nR7LhvknuluSaif3hH463J8O+cb2G0+lJDjfBDvtY7rgvXrvvP7mqdrXWDia5MMkzk9xSVa+pidPIN3C0Y6e1Nqp5PQ9KcvqaPHh6kvuP99/huCzHWCU22tTxU1Xdrap+p4ZLztyWIWvvvaZptXbsE7O1Yzk6o6nFor07wye9PzV5Y1XdPcMy38snbj7ayqubkkyeT33XDKdHTOMFGT7Nf0hr7V4ZdtI15XNtqIbrZ/1yhk8zThlPO/mLNWOdMfH4b8nwGm+cdS0AO1Vr7WMZLhj/mKw51X30qgyneJzRWvvWDNe4WrvPP2Pi6wdm4/3wFRlOO3zk+PW7kjwiQ1Prig22Odaq4k0bP4l/VpJ/nzu+hhszHIQk+auM/basWbU2Psd7W2uPS3K/JG9M8trxrhsyfPJ+74n/7tZae/Ws6gdYRq21K5K8NMlzx5s+k6HR8tcn9offOl5UPhn2l+tdB/HGJKdW1T0nbntg1tkXb1DHq1prP5Rhf94ynAKZbJwja2//YoZm3GF/beLrjWpe73luSPLRNXlwz9baY8b7b8qdc3NWLs5wVsvDx+O0Hx5vX/f4aRz7a7njQgl2GE0tFmpcUvqsJP+pqh5dVSeOp0O8LsOqpJdv8qkuTfLYqvrb44UEn5XpG1H3zHBdlC9U1fck+RdTPs9mxvl6kk8n2VVVv5rh9JRJf6uqfmpcyXZhhgbgldtUD8BO9ZQkPzqeirDWPTN8cv7l8Too/3Sdx/zK+OnwX0/yz5IcWG+Q1tqHMxzo/EySd7bWbsvwafM/ysZNrZuTfHtt8g+SbMLLM5yi8uiJ216V5J9V1VnjNbCek+Sq8RT7v1JVd6mqn66qbx0bZLdlOOUlGVaB7RtXtlVV3b2Gi+xPHpwB7FTPT/KoqjprXHX7oiSXVNX9kqSqHjBxTakXZ9jnnltV3zLe9z3jqXj/K8mvV9XJNfyxjadkzbWC11NV311VPzruw7+cIWsO759vTvLgOvZfOLw2yRPG4609GS7fctgrk/y9qnp8Ve2q4Q9dHT7t/eYk3znx2Pckua2GC9fftapOqKrvq6rDF4R/bZJ/V8NF3b89yb861uvbgntmeO2fHy8A/4x1HvMzVfXQqrpbkl9Lcul4Oic7lKYWC9da+60Mq6Gem+F/oK/K8AnAueO1PzbzHB/KsMN8TYZPB27PcO77prZf45cyHNTcniGw1j14mYHLMpwj/ucZlsZ+OXdeJvymDBc2/lyG8+1/ahPnxAMwYbzO4dUb3P0LSX5tvG7Kr+bIyqRJV2S4QPDlSZ7bWnvrUYa7IslnW2sfn/i+kvzvDR7/P5J8KMmnquq4P0ke/8f9GRn+wMjh2y7PcF2R12fIyN05cu2TtZ6Y5PrxtI59Gf8K8Th/P5fhgryfyzAfTz7eegF60Fr7dIZT2H9lvOmXM+wHrxz3l2/PeF3c1tp7MnwAckmGszCuyJHVsucneXCGVVu/n+QZrbW3baKEk5L8RoYVR5/KsJr26eN9rxv//WxVvW+dbQ/7lQz7/89lWADwqonX9/EMK5ovTnJrhgbY4et5vTjDtbw+X1VvHHPmsRmu9fjRsabfzXAZlYzPfXiV9Fuz+UUKm/H8JHcdx7wyw2mfa708w8q6T2W4HvG/Xucx7CDV2sxWvcPSqKp7JPl8hlMIP7roeqZRw5+VP7O19jOLrgVgFY0rhz+a5MS2/h80AQCWRFW9I8krWmu/u+hamB8rtdgxquqx4+khd8+w6usDOfLncwEAAIAdRFOLneRxGZby3pjkIUme0CxFBAAAgB3J6YcAAAAAdMdKLQAAAAC6o6kFAAAAQHd2zXOwu9RJ7eTcfZ5DsuS+csbW3w9/45RPb0Mld/aBz913LuNM66QbvrjoElgyt+dzn2mtLfcbd87kDmvJnenJHSZ9OV/MV9tXatF1LBOZw1oyZ3oyh7U2OtaZa1Pr5Nw9D69z5zkkS+7gxedseZv37N2/DZXc2e4D++YyzrTOvOjKRZfAknl7u/Rji65h2cgd1pI705M7TLqqXb7oEpaOzGEtmTM9mcNaGx3rOP0QAAAAgO4cV1Orqh5dVX9WVQer6mmzKgoA1iN3AJgXmQOw/KZualXVCUn+S5IfT/LQJOdX1UNnVRgATJI7AMyLzAHow/Gs1Do7ycHW2kdaa19N8pokj5tNWQBwJ3IHgHmROQAdOJ6m1gOS3DDx/SfG2wBgO8gdAOZF5gB04Hj++uF6f8K33elBVRckuSBJTs7djmM4AFac3AFgXmQOQAeOZ6XWJ5KcMfH9tye5ce2DWmsvbK3taa3tOTEnHcdwAKw4uQPAvMgcgA4cT1PrvUkeUlXfUVV3SfKEJG+eTVkAcCdyB4B5kTkAHZj69MPW2ter6qlJLktyQpKXtNY+NLPKAGCC3AFgXmQOQB+O55paaa29JclbZlQLAByV3AFgXmQOwPI7ntMPAQAAAGAhjmulFsvv4CXnLLqEpbD7wL5FlzBzl9147aJLOKrzTj9r0SUACyB3BnJn/uQOrB6ZM5A58ydzloeVWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdEdTCwAAAIDuaGoBAAAA0B1NLQAAAAC6o6kFAAAAQHd2LboANu/gJecsugSWyO4D+6ba7tDe/fMZ65Ktb3LmRVdufSNg28gdJskdYDvJHCbJHDbLSi0AAAAAuqOpBQAAAEB3NLUAAAAA6I6mFgAAAADd0dQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dm16AJ6d9mN185trN0HzpnbWMts94F9W97m0N79cxmnB8v8ug5eMt17fJqf73mnnzXVWLBocqcPcueIZX5dcgeOTubQG5kzWKXMsVILAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdEdTCwAAAIDuaGoBAAAA0B1NLQAAAAC6o6kFAAAAQHeqtTa3we5Vp7aH17lzG2+rDl5yzqJL6NahvfsXXcJR7T6wb9ElsAOcedGViy7hqN7eLr2mtbZn0XUsE7mzc8kdVsEy585V7fLc1m6tRdexTGTOziVzWAXLnDnJxsc6VmoBAAAA0B1NLQAAAAC6o6kFAAAAQHc0tQAAAADojqYWAAAAAN3R1AIAAACgO5paAAAAAHRHUwsAAACA7mhqAQAAANAdTS0AAAAAuqOpBQAAAEB3NLUAAAAA6E611uY22L3q1PbwOncuYx285Jy5jHNo7/4tb7P7wL5tqGR9y17fMptm7qZlzneuMy+6cm5jvb1dek1rbc/cBuzATsydZSd3pid3mIV55c5V7fLc1m6tuQzWCZkzfzJnejKHWViGYx0rtQAAAADojqYWAAAAAN3ZdTwbV9X1SW5P8o0kX3faCwDbSe4AME9yB2C5HVdTa/R3W2ufmcHzAMBmyB0A5knuACwppx8CAAAA0J3jbWq1JG+tqmuq6oJZFAQARyF3AJgnuQOwxI739MNHtNZurKr7JXlbVf1pa+2dkw8Yd/4XJMnJudtxDgfAipM7AMzTUXNH5gAs1nGt1Gqt3Tj+e0uS309y9jqPeWFrbU9rbc+JOel4hgNgxckdAObpWLkjcwAWa+qmVlXdvaruefjrJD+W5IOzKgwAJskdAOZJ7gAsv+M5/fD+SX6/qg4/z6taa384k6oA4M7kDgDzJHcAltzUTa3W2keSPGyGtQDAhuQOAPMkdwCW3/H+9UMAAAAAmDtNLQAAAAC6U621uQ12rzq1PbzOnctYl9147Za32X1g3zZUAv07tHf/lreZ5+/TNPVNY5rXdOZFV25DJet7e7v0mtbanrkN2IGdmDvL/vsIs7Ds73O5k1zVLs9t7daay2Cd2ImZA6tA5gyWOXOSjY91rNQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdEdTCwAAAIDuaGoBAAAA0J1diy7gWA5ecs5U2+0+MN12W3Vo7/4tb7P7wL5tqKRP5m/+duKcz6u+qeYu09V25kVXTrUdx2/Zc4fjsxP3gctuJ8653GFWlj1zduLv7zyZv/nbiXMuc47OSi0AAAAAuqOpBQAAAEB3NLUAAAAA6I6mFgAAAADd0dQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dm16AJ6t/vAvkWX0DXzNzi0d/+Wt5l27sw59M3v8PExfwO50wdzx6J5Dx4f8zeQOWwnK7UAAAAA6I6mFgAAAADd0dQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdKdaa3Mb7KQHntFOv/jCLW1zaO/+qcbafWDfVNux/KZ5T3g/0Jtp3ucnnHbwmtbanm0op1tyh1mQO6yCrb7Pzz7vhlz9/i/XNpXTpT0PO7m957Iz5jKWfczOJXNYBbM81rFSCwAAAIDuaGoBAAAA0B1NLQAAAAC6o6kFAAAAQHc0tQAAAADojqYWAAAAAN3R1AIAAACgO5paAAAAAHRHUwsAAACA7mhqAQAAANAdTS0AAAAAuqOpBQAAAEB3NLUAAAAA6E611uY22J6Hndzec9kZW9pm94F921QNsFmH9u6fyzjz/H3fia/p+gt/6ZrW2p65DdgBuQN92on76J32mm583vPzlY/fUHMZrBMyB/q00/bPyc58TRsd61ipBQAAAEB3jtnUqqqXVNUtVfXBidtOraq3VdWHx39P2d4yAVgVcgeAeZI7AP3azEqtlyZ59Jrbnpbk8tbaQ5JcPn4PALPw0sgdAObnpZE7AF06ZlOrtfbOJLeuuflxSV42fv2yJD8x47oAWFFyB4B5kjsA/Zr2mlr3b63dlCTjv/ebXUkAcCdyB4B5kjsAHdj2C8VX1QVVdXVVXf3pz35ju4cDYMXJHQDmReYALNa0Ta2bq+q0JBn/vWWjB7bWXtha29Na23PfbzthyuEAWHFyB4B52lTuyByAxZq2qfXmJE8av35SkjfNphwAWJfcAWCe5A5AB47Z1KqqVyd5d5LvrqpPVNVTkvxGkkdV1YeTPGr8HgCOm9wBYJ7kDkC/dh3rAa218ze469wZ1wIAcgeAuZI7AP3a9gvFAwAAAMCsaWoBAAAA0J1jnn44Sx/43H2z+8C+eQ65oxzau3/L28xrvqepbVreQ/O37HO+zL8bLJbcOT7L/Lsld3a2nTjnO/E1cUcy5/jInIH3UB+W/ee07PXNkpVaAAAAAHRHUwsAAACA7mhqAQAAANAdTS0AAAAAuqOpBQAAAEB3NLUAAAAA6I6mFgAAAADd0dQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqza9EFbJdDe/dveZvdB/ZtQyWzs8z1zbO2aX6287TMP6dpLfvv006cc/qz7L8n01jm+uTOEcv8cwK2h8yZL5lzxDL/nKa17K9pJ/6+z5KVWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdEdTCwAAAIDuaGoBAAAA0B1NLQAAAAC6U621uQ2252Ent/dcdsaWttl9YN82VQPb49De/Vvexvt8MM3cTWtecz7P13TCaQevaa3tmduAHZA7rAK5Mz25M72zz7shV7//yzWXwTohc1gFMmd6Muf4bHSsY6UWAAAAAN3R1AIAAACgO5paAAAAAHRHUwsAAACA7mhqAQAAANAdTS0AAAAAuqOpBQAAAEB3NLUAAAAA6I6mFgAAAADd0dQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQnV2LLgB2mt0H9i26hKM6tHf/okvY0LLP3TSmfU3L/HMClsuy7zt34v5smedc7gDbaZn3f8n89mXTzMOyz900liFzrNQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdEdTCwAAAIDuaGoBAAAA0J1qrc1tsJMeeEY7/eIL5zbeVh3au3/L2+w+sG8bKlkd5rwPfk59uP7CX7qmtbZn0XUsE7nDWua8D35Oy+/G5z0/X/n4DbXoOpaJzGEtc94HP6c+bHSsY6UWAAAAAN3R1AIAAACgO8dsalXVS6rqlqr64MRtz6yqT1bVteN/j9neMgFYFXIHgHmROQB928xKrZcmefQ6t1/SWjtr/O8tsy0LgBX20sgdAObjpZE5AN06ZlOrtfbOJLfOoRYAkDsAzI3MAejb8VxT66lV9cfjkt1TZlYRAKxP7gAwLzIHoAPTNrVekGR3krOS3JTkeRs9sKouqKqrq+rqb3zhi1MOB8CKkzsAzIvMAejEVE2t1trNrbVvtNa+meRFSc4+ymNf2Frb01rbc8I97j5tnQCsMLkDwLzIHIB+TNXUqqrTJr79ySQf3OixAHC85A4A8yJzAPqx61gPqKpXJ3lkkvtU1SeSPCPJI6vqrCQtyfVJfn4bawRghcgdAOZF5gD07ZhNrdba+evc/OJtqAUA5A4AcyNzAPp2PH/9EAAAAAAWolprcxvspAee0U6/+MK5jQfQq0N79295mxNOO3hNa23PNpTTLbkDsDlbzZ2zz7shV7//y7VN5XRJ5gBsziyPdazUAgAAAKA7mloAAAAAdEdTCwAAAIDuaGoBAAAA0B1NLQAAAAC6o6kFAAAAQHc0tQAAAADojqYWAAAAAN3R1AIAAACgO5paAAAAAHRHUwsAAACA7mhqAQAAANAdTS0AAAAAurNrnoOddMMXc+ZFV25pm4OXnDPVWIf27t/yNrsP7JtqrGU2zTxMYyfOHUf4fZq/804/a4qtDs68jt7JnfmTO8yC36f522ru/Hn77DZV0i+ZM38yh1nw+zR/szzWsVILAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdEdTCwAAAIDuaGoBAAAA0B1NLQAAAAC6o6kFAAAAQHd2LbqA7bL7wL5Fl7AUzMPg0N79W95mnnO37PV5Hw2W/efEYvlZD8zDYNn3F8ten/fRYNl/TiyOn/PAPAyWfV+x7PV5Hw2W/ee0ESu1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdEdTCwAAAIDuaGoBAAAA0B1NLQAAAAC6o6kFAAAAQHc0tQAAAADojqYWAAAAAN3R1AIAAACgO5paAAAAAHRn16ILOJYzL7pyqu0OXnLOjCvhaA7t3T/VdrsP7JvLWNOMw8427Xt2q6bdh7E4codZkDusJXdYj8xhFmQOa61S5lipBQAAAEB3NLUAAAAA6I6mFgAAAADd0dQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA71Vqb22D3qlPbw+vcuYx12Y3Xbnmb3Qf2bUMlwE5xaO/+LW9z3ulnbUMl63t7u/Sa1tqeuQ3YAbkD9GyZc+eqdnlua7fWXAbrhMwBerbMmZNsfKxjpRYAAAAA3dHUAgAAAKA7x2xqVdUZVfVHVXVdVX2oqn5xvP3UqnpbVX14/PeU7S8XgJ1O7gAwLzIHoG+bWan19SQXt9a+N8k5Sf5lVT00ydOSXN5ae0iSy8fvAeB4yR0A5kXmAHTsmE2t1tpNrbX3jV/fnuS6JA9I8rgkLxsf9rIkP26P/MsAAAqASURBVLFdRQKwOuQOAPMicwD6tqVralXVg5N8f5Krkty/tXZTMoRBkvvNujgAVpvcAWBeZA5Afzbd1KqqeyR5fZILW2u3bWG7C6rq6qq6+mv5yjQ1ArCC5A4A8yJzAPq0qaZWVZ2YYSf/ytbaG8abb66q08b7T0tyy3rbttZe2Frb01rbc2JOmkXNAOxwcgeAeZE5AP3azF8/rCQvTnJda+23J+56c5InjV8/KcmbZl8eAKtG7gAwLzIHoG+7NvGYRyR5YpIPVNW1421PT/IbSV5bVU9J8vEk/2R7SgRgxcgdAOZF5gB07JhNrdbau5LUBnefO9tyAFh1cgeAeZE5AH3b0l8/BAAAAIBloKkFAAAAQHc2c02tLp13+llb3+iS2dfB0R3au3/L2+w+sG8bKpmdnfialt285nyq/QorQ+70YSfuo3fia1p2codFkzl92In75534mpadzDk6K7UAAAAA6I6mFgAAAADd0dQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdGfXogtYJmdedOWWtzl4yTlb3ubQ3v1b3iZJdh/YN9V2zJef0/xNM+fT/L7DrMkdZsHPqQ9yh0WTOcyCn9P8OdY5Oiu1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdEdTCwAAAIDuaGoBAAAA0B1NLQAAAAC6o6kFAAAAQHc0tQAAAADojqYWAAAAAN3R1AIAAACgO5paAAAAAHRn16IL6N2ZF1255W12Z982VMKyOLR3/5a32X1gud8T07ymaZx3+llzGQd6JndYS+5Mb9rcOe8iecVqkDmsJXOm51hne1ipBQAAAEB3NLUAAAAA6I6mFgAAAADd0dQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7uxZdwCo686Ir5zbWwUvOmcs4h/bun2q73Qf2zWWseY3DEeedftaiSwBGcqcPO/E1zZPcgeUgc/qwE1/TPMmc5WGlFgAAAADd0dQCAAAAoDuaWgAAAAB0R1MLAAAAgO5oagEAAADQHU0tAAAAALqjqQUAAABAdzS1AAAAAOiOphYAAAAA3dHUAgAAAKA7mloAAAAAdEdTCwAAAIDuaGoBAAAA0J1qrc1tsHvVqe3hde7cxmP5XXbjtVveZveBfVve5tDe/XMZZ55jnXnRlVvehp3t7e3Sa1prexZdxzKRO6w1Te7sRHKH43VVuzy3tVtr0XUsE5nDWjJnIHOYhY2OdazUAgAAAKA7x2xqVdUZVfVHVXVdVX2oqn5xvP2ZVfXJqrp2/O8x218uADud3AFgXmQOQN92beIxX09ycWvtfVV1zyTXVNXbxvsuaa09d/vKA2AFyR0A5kXmAHTsmE2t1tpNSW4av769qq5L8oDtLgyA1SR3AJgXmQPQty1dU6uqHpzk+5NcNd701Kr646p6SVWdMuPaAFhxcgeAeZE5AP3ZdFOrqu6R5PVJLmyt3ZbkBUl2Jzkrw6cbz9tguwuq6uqquvpr+coMSgZgFcgdAOZF5gD0aVNNrao6McNO/pWttTckSWvt5tbaN1pr30zyoiRnr7dta+2FrbU9rbU9J+akWdUNwA4mdwCYF5kD0K/N/PXDSvLiJNe11n574vbTJh72k0k+OPvyAFg1cgeAeZE5AH3bzF8/fESSJyb5QFVdO9729CTnV9VZSVqS65P8/LZUCMCqkTsAzIvMAejYZv764buS1Dp3vWX25QCw6uQOAPMicwD6tqW/fggAAAAAy0BTCwAAAIDuVGttfoNVfTrJxza4+z5JPjO3YpaXeRiYh4F5OMJcDI42Dw9qrd13nsUsu6PkjvfTwDwcYS4G5mFgHo7YaC5kzhqOdTbFPAzMwxHmYmAeBls+1plrU+toqurq1tqeRdexaOZhYB4G5uEIczEwD7NhHgfm4QhzMTAPA/NwhLmYDfM4MA8D83CEuRiYh8E08+D0QwAAAAC6o6kFAAAAQHeWqan1wkUXsCTMw8A8DMzDEeZiYB5mwzwOzMMR5mJgHgbm4QhzMRvmcWAeBubhCHMxMA+DLc/D0lxTCwAAAAA2a5lWagEAAADApiy8qVVVj66qP6uqg1X1tEXXsyhVdX1VfaCqrq2qqxddzzxV1Uuq6paq+uDEbadW1duq6sPjv6csssZ52GAenllVnxzfF9dW1WMWWeM8VNUZVfVHVXVdVX2oqn5xvH2l3hNHmYeVe0/MmtwZyB25I3cGcmcgd7aHzDliVXNH5gxkzkDmDGaZOQs9/bCqTkjy50keleQTSd6b5PzW2p8srKgFqarrk+xprX1m0bXMW1X9cJIvJPm91tr3jbf9VpJbW2u/Mf4PwCmttV9eZJ3bbYN5eGaSL7TWnrvI2uapqk5Lclpr7X1Vdc8k1yT5iSRPzgq9J44yD4/Pir0nZknuHCF35I7cGcidgdyZPZlzR6uaOzJnIHMGMmcwy8xZ9Eqts5McbK19pLX21SSvSfK4BdfEnLXW3pnk1jU3Py7Jy8avX5bhDb6jbTAPK6e1dlNr7X3j17cnuS7JA7Ji74mjzAPHR+4gd0ZyZyB3BnJnW8gcZM5I5gxkzmCWmbPoptYDktww8f0nsrrh2ZK8taquqaoLFl3MErh/a+2mZHjDJ7nfgutZpKdW1R+PS3Z39DLUtarqwUm+P8lVWeH3xJp5SFb4PTEDcucIuXNHK7uPWcfK7mPkzkDuzIzMuSO5c8TK7l/WsbL7F5kzON7MWXRTq9a5bVX/HOMjWmt/M8mPJ/mX4/JMeEGS3UnOSnJTkucttpz5qap7JHl9kgtba7ctup5FWWceVvY9MSNy5wi5w3pWdh8jdwZyZ6Zkzh3JHdZa2f2LzBnMInMW3dT6RJIzJr7/9iQ3LqiWhWqt3Tj+e0uS38+wXHmV3TyeZ3v4fNtbFlzPQrTWbm6tfaO19s0kL8qKvC+q6sQMO7dXttbeMN68cu+J9eZhVd8TMyR3RnLnTlZuH7OeVd3HyJ2B3Jk5mTNB7tzByu1f1rOq+xeZM5hV5iy6qfXeJA+pqu+oqrskeUKSNy+4prmrqruPF0dLVd09yY8l+eDRt9rx3pzkSePXT0rypgXWsjCHd2yjn8wKvC+qqpK8OMl1rbXfnrhrpd4TG83DKr4nZkzuRO5sYKX2MRtZxX2M3BnInW0hc0Zy505Wav+ykVXcv8icwSwzZ6F//TBJavgTjc9PckKSl7TWnr3Qghagqr4zw6cVSbIryatWaR6q6tVJHpnkPkluTvKMJG9M8tokD0zy8ST/pLW2oy8suME8PDLD0suW5PokP3/4XOudqqp+KMn/TPKBJN8cb356hnOsV+Y9cZR5OD8r9p6YNbkjd+TOQO4M5M5A7mwPmTNY5dyROQOZM5A5g1lmzsKbWgAAAACwVYs+/RAAAAAAtkxTCwAAAIDuaGoBAAAA0B1NLQAAAAC6o6kFAAAAQHc0tQAAAADojqYWAAAAAN3R1AIAAACgO/8/0qDsCfU5Dj0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1512x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(21, 7))\n",
    "\n",
    "ax[0].imshow(np.argmax(x[3], axis=2));\n",
    "ax[0].set_title(\"Original map\")\n",
    "noised_gen_x = np.argmax(decoder.predict(noised_encoded_x), axis=3)\n",
    "ax[1].imshow(noised_gen_x[3])\n",
    "ax[1].set_title(\"Map with Noise\")\n",
    "gen_x = np.argmax(ae.predict(x), axis=3)\n",
    "ax[2].imshow(gen_x[3])\n",
    "ax[2].set_title(\"Reconstructed map\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "true"
   },
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train_ae.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train_ae.py\n",
    "import os\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import argparse\n",
    "from tensorflow.keras import layers, Input, models\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument(\"--num-epochs\", type=int, default=15)\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=1024)\n",
    "    parser.add_argument(\"--max-rows\", type=int, default=None)\n",
    "    parser.add_argument(\"--model_dir\", type=str, default=None)\n",
    "    parser.add_argument('--model-save-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "#    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "    return parser.parse_args()\n",
    "\n",
    "    \n",
    "def create_autoencoder(data, epochs, batch_size):\n",
    "    input_shape = (26, 26, 3)\n",
    "    input_tensor = Input(input_shape)\n",
    "    encode = layers.Conv2D(64, (3,3), padding='same', activation='relu')(input_tensor)\n",
    "\n",
    "    latent_vector = layers.MaxPool2D()(encode)\n",
    "\n",
    "    # Decoder\n",
    "    decode_layer_1 = layers.Conv2DTranspose(64, (3,3), padding='same', activation='relu')\n",
    "    decode_layer_2 = layers.UpSampling2D()\n",
    "    output_tensor = layers.Conv2DTranspose(3, (3,3), padding='same', activation='sigmoid')\n",
    "\n",
    "    # connect decoder layers\n",
    "    decode = decode_layer_1(latent_vector)\n",
    "    decode = decode_layer_2(decode)\n",
    "\n",
    "    ae = models.Model(input_tensor, output_tensor(decode))\n",
    "    ae.compile(\n",
    "        optimizer = 'Adam',\n",
    "        loss = 'mse',\n",
    "    )\n",
    "    ae.fit(data, data,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        verbose=2)\n",
    "    \n",
    "    decoder_input = Input((13, 13, 64))\n",
    "    decoder = decode_layer_1(decoder_input)\n",
    "    decoder = decode_layer_2(decoder)\n",
    "    \n",
    "    return ae, models.Model(input_tensor, latent_vector), models.Model(decoder_input, output_tensor(decoder))\n",
    "\n",
    "\n",
    "def load_data(path: str, file_name: str=\"data.npz\", limit: int=None):\n",
    "    file_path = Path(path) / file_name\n",
    "    with np.load(str(file_path), allow_pickle=True) as data:\n",
    "        x = data['x']\n",
    "        y = data['y']\n",
    "    if limit:\n",
    "        return (x[:limit], y[:limit])\n",
    "    else:\n",
    "        return (x, y)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_arguments()\n",
    "    x, y = load_data(\n",
    "        args.train, \n",
    "        \"data.npz\",\n",
    "        limit=args.max_rows\n",
    "    )\n",
    "    ae, encoder, decoder = create_autoencoder(x, epochs=15, batch_size=1024)\n",
    "    ae.save(str(Path(args.model_save_dir) / \"ae.h5\" ))\n",
    "    encoder.save(str(Path(args.model_save_dir) / \"encoder.h5\" ))\n",
    "    decoder.save(str(Path(args.model_save_dir) / \"decoder.h5\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Run on Container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-east-1-160951647621/data-augmentation-2020-10-05-00-02-40-520/output/classifier/train'}\n"
     ]
    }
   ],
   "source": [
    "bucket = \"sagemaker-us-east-1-160951647621\"\n",
    "data_path = \"wafer-data-processing-2020-10-04-21-48-20-207/output/autoencoder/train\"\n",
    "\n",
    "inputs = {\n",
    "    \"train\": f\"s3://{bucket}/{data_path}\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Instance and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_instance_type = \"ml.p3.2xlarge\"\n",
    "hyperparameters = {\n",
    "    #\"max-rows\": 1024,\n",
    "    \"num-epochs\": 15,\n",
    "    \"batch-size\": 1024,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Estimator Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.72.1'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='src/train_ae.py',\n",
    "                       base_job_name='train-autoencoder',\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(), # Passes to the container the AWS role that you are using on this notebook\n",
    "                       framework_version='2.1.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-04 23:23:27 Starting - Starting the training job...\n",
      "2020-10-04 23:23:29 Starting - Launching requested ML instances......\n",
      "2020-10-04 23:24:51 Starting - Preparing the instances for training.........\n",
      "2020-10-04 23:26:06 Downloading - Downloading input data\n",
      "2020-10-04 23:26:06 Training - Downloading the training image......\n",
      "2020-10-04 23:27:18 Training - Training image download completed. Training in progress..\u001b[34m2020-10-04 23:27:23,010 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-10-04 23:27:23,448 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 1024,\n",
      "        \"num-epochs\": 15,\n",
      "        \"model_dir\": \"s3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-04-23-08-39-857/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"tensorflow-training-2020-10-04-23-23-27-366\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-04-23-23-27-366/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_ae\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_ae.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":1024,\"model_dir\":\"s3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-04-23-08-39-857/model\",\"num-epochs\":15}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_ae.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_ae\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-04-23-23-27-366/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":1024,\"model_dir\":\"s3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-04-23-08-39-857/model\",\"num-epochs\":15},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"tensorflow-training-2020-10-04-23-23-27-366\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-04-23-23-27-366/source/sourcedir.tar.gz\",\"module_name\":\"train_ae\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ae.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"1024\",\"--model_dir\",\"s3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-04-23-08-39-857/model\",\"--num-epochs\",\"15\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=1024\u001b[0m\n",
      "\u001b[34mSM_HP_NUM-EPOCHS=15\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-04-23-08-39-857/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train_ae.py --batch-size 1024 --model_dir s3://sagemaker-us-east-1-160951647621/tensorflow-training-2020-10-04-23-08-39-857/model --num-epochs 15\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m(array(['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Near-full',\n",
      "       'Random', 'Scratch', 'none'], dtype=object), array([  803,    14,   680,  1093,   634,    32,   282,   165, 19191]))\u001b[0m\n",
      "\u001b[34m[2020-10-04 23:27:29.584 ip-10-0-229-89.ec2.internal:27 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-10-04 23:27:29.585 ip-10-0-229-89.ec2.internal:27 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-10-04 23:27:29.585 ip-10-0-229-89.ec2.internal:27 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34mTrain on 22894 samples\u001b[0m\n",
      "\u001b[34m[2020-10-04 23:27:30.255 ip-10-0-229-89.ec2.internal:27 INFO keras.py:68] Executing in TF2.x eager mode.TF 2.x eager doesn't provide gradient and optimizer variable values.SageMaker Debugger will not be saving gradients and optimizer variables in this case\u001b[0m\n",
      "\u001b[34mEpoch 1/15\u001b[0m\n",
      "\u001b[34m[2020-10-04 23:27:30.266 ip-10-0-229-89.ec2.internal:27 INFO hook.py:364] Monitoring the collections: losses, metrics, sm_metrics\u001b[0m\n",
      "\u001b[34mERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m22894/22894 - 6s - loss: 0.1268\u001b[0m\n",
      "\u001b[34mEpoch 2/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0737\u001b[0m\n",
      "\u001b[34mEpoch 3/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0630\u001b[0m\n",
      "\u001b[34mEpoch 4/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0507\u001b[0m\n",
      "\u001b[34mEpoch 5/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 1s - loss: 0.0434\u001b[0m\n",
      "\u001b[34mEpoch 6/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0379\u001b[0m\n",
      "\u001b[34mEpoch 7/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 1s - loss: 0.0328\u001b[0m\n",
      "\u001b[34mEpoch 8/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0277\u001b[0m\n",
      "\u001b[34mEpoch 9/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0234\u001b[0m\n",
      "\u001b[34mEpoch 10/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0199\u001b[0m\n",
      "\u001b[34mEpoch 11/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0173\u001b[0m\n",
      "\u001b[34mEpoch 12/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0154\u001b[0m\n",
      "\u001b[34mEpoch 13/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0138\u001b[0m\n",
      "\u001b[34mEpoch 14/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0126\u001b[0m\n",
      "\u001b[34mEpoch 15/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0115\u001b[0m\n",
      "\u001b[34mModel: \"model\"\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34minput_1 (InputLayer)         [(None, 26, 26, 3)]       0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d (Conv2D)              (None, 26, 26, 64)        1792      \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mmax_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d_transpose (Conv2DTran (None, 13, 13, 64)        36928     \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mup_sampling2d (UpSampling2D) (None, 26, 26, 64)        0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d_transpose_1 (Conv2DTr (None, 26, 26, 3)         1731      \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 40,451\u001b[0m\n",
      "\u001b[34mTrainable params: 40,451\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mINFO:root:Autoencoder: None\u001b[0m\n",
      "\u001b[34mModel: \"model_1\"\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34minput_1 (InputLayer)         [(None, 26, 26, 3)]       0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d (Conv2D)              (None, 26, 26, 64)        1792      \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mmax_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 1,792\u001b[0m\n",
      "\u001b[34mTrainable params: 1,792\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mINFO:root:Encoder: None\u001b[0m\n",
      "\u001b[34mModel: \"model_2\"\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34minput_2 (InputLayer)         [(None, 13, 13, 64)]      0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d_transpose (Conv2DTran (None, 13, 13, 64)        36928     \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mup_sampling2d (UpSampling2D) (None, 26, 26, 64)        0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d_transpose_1 (Conv2DTr (None, 26, 26, 3)         1731      \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 38,659\u001b[0m\n",
      "\u001b[34mTrainable params: 38,659\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mINFO:root:Dncoder: None\u001b[0m\n",
      "\u001b[34m[2020-10-04 23:27:43.148 ip-10-0-229-89.ec2.internal:27 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-10-04 23:27:44,274 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container. The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[34mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n",
      "\u001b[34m2020-10-04 23:27:44,275 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-10-04 23:27:56 Uploading - Uploading generated training model\n",
      "2020-10-04 23:27:56 Completed - Training job completed\n",
      "Training seconds: 117\n",
      "Billable seconds: 117\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "estimator.model_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
