{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Training the Autoencoder\n",
    "\n",
    "Due to SageMaker having native support for Tensorflow, training the autoencoder is very simple - we only need to provide the script and point a [Tensorflow Estimator](https://sagemaker.readthedocs.io/en/stable/frameworks/tensorflow/using_tf.html#train-a-model-with-tensorflow) to it and the data. Note that tensorflow (and all its dependencies) doesn't even have to be available to the notebook itself - it will only be used inside the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The script is an edited version of the code created in the original notebook. The main difference is that the hyperparameters (and any others) are passed to it through the command line, using the `argparse` (or any other CLI parameter parser) module.\n",
    "\n",
    "Another slight difference (also handled through parameters and environment variables) is that SageMaker defines specific directories where the data will be made available to the script and where the trained model should be saved. With that, the script doesn't have to know about S3 or any data movement outside of its environment - it's completely self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36msagemaker\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mnumpy\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mnp\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m\u001b[04m\u001b[36m.\u001b[39;49;00m\u001b[04m\u001b[36mkeras\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m layers, Input, models\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mpathlib\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m Path\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mlogging\u001b[39;49;00m\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_arguments\u001b[39;49;00m():\n",
      "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num-epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m15\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--batch-size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m1024\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max-rows\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--model-save-dir\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m])\n",
      "\u001b[37m#    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\u001b[39;49;00m\n",
      "    \u001b[34mreturn\u001b[39;49;00m parser.parse_args()\n",
      "\n",
      "    \n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mcreate_autoencoder\u001b[39;49;00m(data, epochs, batch_size):\n",
      "    input_shape = (\u001b[34m26\u001b[39;49;00m, \u001b[34m26\u001b[39;49;00m, \u001b[34m3\u001b[39;49;00m)\n",
      "    input_tensor = Input(input_shape)\n",
      "    encode = layers.Conv2D(\u001b[34m64\u001b[39;49;00m, (\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m'\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)(input_tensor)\n",
      "\n",
      "    latent_vector = layers.MaxPool2D()(encode)\n",
      "\n",
      "    \u001b[37m# Decoder\u001b[39;49;00m\n",
      "    decode_layer_1 = layers.Conv2DTranspose(\u001b[34m64\u001b[39;49;00m, (\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m'\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33mrelu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    decode_layer_2 = layers.UpSampling2D()\n",
      "    output_tensor = layers.Conv2DTranspose(\u001b[34m3\u001b[39;49;00m, (\u001b[34m3\u001b[39;49;00m,\u001b[34m3\u001b[39;49;00m), padding=\u001b[33m'\u001b[39;49;00m\u001b[33msame\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, activation=\u001b[33m'\u001b[39;49;00m\u001b[33msigmoid\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# connect decoder layers\u001b[39;49;00m\n",
      "    decode = decode_layer_1(latent_vector)\n",
      "    decode = decode_layer_2(decode)\n",
      "\n",
      "    ae = models.Model(input_tensor, output_tensor(decode))\n",
      "    ae.compile(\n",
      "        optimizer = \u001b[33m'\u001b[39;49;00m\u001b[33mAdam\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "        loss = \u001b[33m'\u001b[39;49;00m\u001b[33mmse\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m,\n",
      "    )\n",
      "    ae.fit(data, data,\n",
      "        batch_size=batch_size,\n",
      "        epochs=epochs,\n",
      "        verbose=\u001b[34m2\u001b[39;49;00m)\n",
      "    \n",
      "    decoder_input = Input((\u001b[34m13\u001b[39;49;00m, \u001b[34m13\u001b[39;49;00m, \u001b[34m64\u001b[39;49;00m))\n",
      "    decoder = decode_layer_1(decoder_input)\n",
      "    decoder = decode_layer_2(decoder)\n",
      "    \n",
      "    \u001b[34mreturn\u001b[39;49;00m ae, models.Model(input_tensor, latent_vector), models.Model(decoder_input, output_tensor(decoder))\n",
      "\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mload_data\u001b[39;49;00m(path: \u001b[36mstr\u001b[39;49;00m, file_name: \u001b[36mstr\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33mdata.npz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, limit: \u001b[36mint\u001b[39;49;00m=\u001b[34mNone\u001b[39;49;00m):\n",
      "    file_path = Path(path) / file_name\n",
      "    \u001b[34mwith\u001b[39;49;00m np.load(\u001b[36mstr\u001b[39;49;00m(file_path), allow_pickle=\u001b[34mTrue\u001b[39;49;00m) \u001b[34mas\u001b[39;49;00m data:\n",
      "        x = data[\u001b[33m'\u001b[39;49;00m\u001b[33mx\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "        y = data[\u001b[33m'\u001b[39;49;00m\u001b[33my\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \u001b[36mprint\u001b[39;49;00m(np.unique(y, return_counts=\u001b[34mTrue\u001b[39;49;00m))\n",
      "    \u001b[34mif\u001b[39;49;00m limit:\n",
      "        \u001b[34mreturn\u001b[39;49;00m (x[:limit], y[:limit])\n",
      "    \u001b[34melse\u001b[39;49;00m:\n",
      "        \u001b[34mreturn\u001b[39;49;00m (x, y)\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    logging.basicConfig(level=logging.INFO)\n",
      "    args = parse_arguments()\n",
      "    x, y = load_data(args.train, \u001b[33m\"\u001b[39;49;00m\u001b[33mdata.npz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, limit=args.max_rows)\n",
      "    ae, encoder, decoder = create_autoencoder(x, epochs=args.num_epochs, batch_size=args.batch_size)\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mAutoencoder: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mae.summary()\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mEncoder: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mencoder.summary()\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    logging.info(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mDncoder: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdecoder.summary()\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    ae.save(\u001b[36mstr\u001b[39;49;00m(Path(args.model_save_dir) / \u001b[33m\"\u001b[39;49;00m\u001b[33mae.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m ))\n",
      "    encoder.save(\u001b[36mstr\u001b[39;49;00m(Path(args.model_save_dir) / \u001b[33m\"\u001b[39;49;00m\u001b[33mencoder.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m ))\n",
      "    decoder.save(\u001b[36mstr\u001b[39;49;00m(Path(args.model_save_dir) / \u001b[33m\"\u001b[39;49;00m\u001b[33mdecoder.h5\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m ))\n"
     ]
    }
   ],
   "source": [
    "!pygmentize src/train_ae.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Run the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As this is a model training, there are some additional parameters to pass to it. This examples only defined the number of epochs and batch size, but any hyperparameter could be passed the same way. [SageMaker Hyperparameter Optimization](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) (SageMaker Automatic Tuning) uses the same strategy to optimize trained models over their possible hyperparameter ranges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Input Data Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': 's3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-10-04-21-48-20-207/output/autoencoder/train'}\n"
     ]
    }
   ],
   "source": [
    "bucket = \"sagemaker-us-east-1-160951647621\"\n",
    "data_path = \"wafer-data-processing-2020-10-04-21-48-20-207/output/autoencoder/train\"\n",
    "\n",
    "inputs = {\n",
    "    \"train\": f\"s3://{bucket}/{data_path}\"\n",
    "}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Instance and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "train_instance_type = \"ml.p3.2xlarge\"\n",
    "hyperparameters = {\n",
    "    #\"max-rows\": 1024,\n",
    "    \"num-epochs\": 15,\n",
    "    \"batch-size\": 1024,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Estimator Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "estimator = TensorFlow(entry_point='src/train_ae.py',\n",
    "                       base_job_name='train-autoencoder',\n",
    "                       train_instance_type=train_instance_type,\n",
    "                       train_instance_count=1,\n",
    "                       hyperparameters=hyperparameters,\n",
    "                       role=sagemaker.get_execution_role(), # Passes to the container the AWS role that you are using on this notebook\n",
    "                       framework_version='2.1.0',\n",
    "                       py_version='py3',\n",
    "                       script_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "'create_image_uri' will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-10-05 21:46:59 Starting - Starting the training job...\n",
      "2020-10-05 21:47:03 Starting - Launching requested ML instances......\n",
      "2020-10-05 21:48:08 Starting - Preparing the instances for training......\n",
      "2020-10-05 21:49:16 Downloading - Downloading input data\n",
      "2020-10-05 21:49:16 Training - Downloading the training image.........\n",
      "2020-10-05 21:50:48 Training - Training image download completed. Training in progress.\u001b[34m2020-10-05 21:50:52,198 sagemaker-containers INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2020-10-05 21:50:52,630 sagemaker-containers INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch-size\": 1024,\n",
      "        \"num-epochs\": 15,\n",
      "        \"model_dir\": \"s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/model\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"train-autoencoder-2020-10-05-21-46-58-914\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train_ae\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train_ae.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch-size\":1024,\"model_dir\":\"s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/model\",\"num-epochs\":15}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train_ae.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train_ae\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch-size\":1024,\"model_dir\":\"s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/model\",\"num-epochs\":15},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"train-autoencoder-2020-10-05-21-46-58-914\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/source/sourcedir.tar.gz\",\"module_name\":\"train_ae\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train_ae.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch-size\",\"1024\",\"--model_dir\",\"s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/model\",\"--num-epochs\",\"15\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH-SIZE=1024\u001b[0m\n",
      "\u001b[34mSM_HP_NUM-EPOCHS=15\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_DIR=s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/model\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/lib/python36.zip:/usr/lib/python3.6:/usr/lib/python3.6/lib-dynload:/usr/local/lib/python3.6/dist-packages:/usr/lib/python3/dist-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/usr/bin/python3 train_ae.py --batch-size 1024 --model_dir s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/model --num-epochs 15\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m(array(['Center', 'Donut', 'Edge-Loc', 'Edge-Ring', 'Loc', 'Near-full',\n",
      "       'Random', 'Scratch', 'none'], dtype=object), array([  803,    14,   680,  1093,   634,    32,   282,   165, 19191]))\u001b[0m\n",
      "\u001b[34m[2020-10-05 21:50:58.448 ip-10-0-226-239.ec2.internal:28 INFO json_config.py:90] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2020-10-05 21:50:58.448 ip-10-0-226-239.ec2.internal:28 INFO hook.py:183] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2020-10-05 21:50:58.448 ip-10-0-226-239.ec2.internal:28 INFO hook.py:228] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34mTrain on 22894 samples\u001b[0m\n",
      "\u001b[34m[2020-10-05 21:50:59.036 ip-10-0-226-239.ec2.internal:28 INFO keras.py:68] Executing in TF2.x eager mode.TF 2.x eager doesn't provide gradient and optimizer variable values.SageMaker Debugger will not be saving gradients and optimizer variables in this case\u001b[0m\n",
      "\u001b[34mEpoch 1/15\u001b[0m\n",
      "\u001b[34m[2020-10-05 21:50:59.046 ip-10-0-226-239.ec2.internal:28 INFO hook.py:364] Monitoring the collections: losses, metrics, sm_metrics\u001b[0m\n",
      "\u001b[34mERROR:root:'NoneType' object has no attribute 'write'\u001b[0m\n",
      "\u001b[34m22894/22894 - 4s - loss: 0.1263\u001b[0m\n",
      "\u001b[34mEpoch 2/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0744\u001b[0m\n",
      "\u001b[34mEpoch 3/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0627\u001b[0m\n",
      "\u001b[34mEpoch 4/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0514\u001b[0m\n",
      "\u001b[34mEpoch 5/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0443\u001b[0m\n",
      "\u001b[34mEpoch 6/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0381\u001b[0m\n",
      "\u001b[34mEpoch 7/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0325\u001b[0m\n",
      "\u001b[34mEpoch 8/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0273\u001b[0m\n",
      "\u001b[34mEpoch 9/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0231\u001b[0m\n",
      "\u001b[34mEpoch 10/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0199\u001b[0m\n",
      "\u001b[34mEpoch 11/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0174\u001b[0m\n",
      "\u001b[34mEpoch 12/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0154\u001b[0m\n",
      "\u001b[34mEpoch 13/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0139\u001b[0m\n",
      "\u001b[34mEpoch 14/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0126\u001b[0m\n",
      "\u001b[34mEpoch 15/15\u001b[0m\n",
      "\u001b[34m22894/22894 - 0s - loss: 0.0115\u001b[0m\n",
      "\u001b[34mModel: \"model\"\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34minput_1 (InputLayer)         [(None, 26, 26, 3)]       0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d (Conv2D)              (None, 26, 26, 64)        1792      \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mmax_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d_transpose (Conv2DTran (None, 13, 13, 64)        36928     \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mup_sampling2d (UpSampling2D) (None, 26, 26, 64)        0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d_transpose_1 (Conv2DTr (None, 26, 26, 3)         1731      \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 40,451\u001b[0m\n",
      "\u001b[34mTrainable params: 40,451\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mINFO:root:Autoencoder: None\u001b[0m\n",
      "\u001b[34mModel: \"model_1\"\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34minput_1 (InputLayer)         [(None, 26, 26, 3)]       0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d (Conv2D)              (None, 26, 26, 64)        1792      \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mmax_pooling2d (MaxPooling2D) (None, 13, 13, 64)        0         \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 1,792\u001b[0m\n",
      "\u001b[34mTrainable params: 1,792\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mINFO:root:Encoder: None\u001b[0m\n",
      "\u001b[34mModel: \"model_2\"\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mLayer (type)                 Output Shape              Param #   \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34minput_2 (InputLayer)         [(None, 13, 13, 64)]      0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d_transpose (Conv2DTran (None, 13, 13, 64)        36928     \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mup_sampling2d (UpSampling2D) (None, 26, 26, 64)        0         \u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mconv2d_transpose_1 (Conv2DTr (None, 26, 26, 3)         1731      \u001b[0m\n",
      "\u001b[34m=================================================================\u001b[0m\n",
      "\u001b[34mTotal params: 38,659\u001b[0m\n",
      "\u001b[34mTrainable params: 38,659\u001b[0m\n",
      "\u001b[34mNon-trainable params: 0\u001b[0m\n",
      "\u001b[34m_________________________________________________________________\u001b[0m\n",
      "\u001b[34mINFO:root:Dncoder: None\u001b[0m\n",
      "\u001b[34m[2020-10-05 21:51:09.683 ip-10-0-226-239.ec2.internal:28 INFO utils.py:25] The end of training job file will not be written for jobs running under SageMaker.\u001b[0m\n",
      "\u001b[34m2020-10-05 21:51:10,675 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container. The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[34mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n",
      "\u001b[34m2020-10-05 21:51:10,676 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2020-10-05 21:51:21 Uploading - Uploading generated training model\n",
      "2020-10-05 21:51:21 Completed - Training job completed\n",
      "Training seconds: 132\n",
      "Billable seconds: 132\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The message above that \"Your model will NOT be servable with SageMaker TensorFlow Serving container\" is not a concern. We saved the models as Keras `.h5` files, and we'll use them for data processing, not inference. The models are compacted by SageMaker and made available at this location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-160951647621/train-autoencoder-2020-10-05-21-46-58-914/output/model.tar.gz'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "estimator.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "With the encoder and decoder ready for use, we can proceed to [data augmentation](data_aug.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
