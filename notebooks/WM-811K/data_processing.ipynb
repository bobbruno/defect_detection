{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#import dask.dataframe as dd\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import cv2\n",
    "import os\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "root_path = Path('/home/ec2-user/SageMaker/defect_detection/')#.resolve()\n",
    "\n",
    "code_path = root_path / \"notebooks/WM-811K/src/\"\n",
    "code_path.mkdir(exist_ok=True)\n",
    "data_path = root_path / \"data/MIR-WM811K/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Creating SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Build a Container for Dask Processing\n",
    "\n",
    "Create a container for processing with Dask. The code below is based on [this example](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/feature_transformation_with_sagemaker_processing_dask/feature_transformation_with_sagemaker_processing_dask.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/SageMaker/defect_detection/notebooks/WM-811K/src/container ~/SageMaker/defect_detection/notebooks\n",
      "Sending build context to Docker daemon  16.38kB\n",
      "Step 1/21 : FROM continuumio/miniconda3:4.7.12\n",
      " ---> 406f2b43ea59\n",
      "Step 2/21 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> bee7c1789cd8\n",
      "Step 3/21 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil ffmpeg libsm6 libxext6\n",
      " ---> Using cache\n",
      " ---> ca8821c9b19a\n",
      "Step 4/21 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> 44a6c1b8958e\n",
      "Step 5/21 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 0b6d3e8f0d43\n",
      "Step 6/21 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 1de28fac9d35\n",
      "Step 7/21 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 9119ae001b0b\n",
      "Step 8/21 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> d8659b8f85c9\n",
      "Step 9/21 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> b5ec0227bc57\n",
      "Step 10/21 : RUN conda install --yes     -c conda-forge     python==3.8     python-blosc     cytoolz     dask==2.16.0     distributed==2.16.0     lz4     opencv     nomkl     numpy==1.18.1     pandas==1.0.5     tini==0.18.0     && conda clean -tipsy     && find /opt/conda/ -type f,l -name '*.a' -delete     && find /opt/conda/ -type f,l -name '*.pyc' -delete     && find /opt/conda/ -type f,l -name '*.js.map' -delete     && find /opt/conda/lib/python*/site-packages/bokeh/server/static -type f,l -name '*.js' -not -name '*.min.js' -delete     && rm -rf /opt/conda/pkgs\n",
      " ---> Using cache\n",
      " ---> cc9323fc7102\n",
      "Step 11/21 : RUN pip install dask-ml\n",
      " ---> Using cache\n",
      " ---> 6bce6338d2e6\n",
      "Step 12/21 : RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64\n",
      " ---> Using cache\n",
      " ---> e734c072da14\n",
      "Step 13/21 : RUN chmod +x /usr/local/bin/dumb-init\n",
      " ---> Using cache\n",
      " ---> 33720a25ad10\n",
      "Step 14/21 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> c1ddb9c3c2a1\n",
      "Step 15/21 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> f2bca6f0af4a\n",
      "Step 16/21 : RUN conda install --yes s3fs -c conda-forge\n",
      " ---> Using cache\n",
      " ---> 45c8823afe93\n",
      "Step 17/21 : RUN mkdir /opt/app /etc/dask\n",
      " ---> Using cache\n",
      " ---> 04d67b588185\n",
      "Step 18/21 : COPY dask_config/dask.yaml /etc/dask/\n",
      " ---> Using cache\n",
      " ---> a326f698422c\n",
      "Step 19/21 : COPY program /opt/program\n",
      " ---> Using cache\n",
      " ---> f2f61e34e2fa\n",
      "Step 20/21 : RUN chmod +x /opt/program/bootstrap.py\n",
      " ---> Using cache\n",
      " ---> 0f21f71bda59\n",
      "Step 21/21 : ENTRYPOINT [\"/opt/program/bootstrap.py\"]\n",
      " ---> Using cache\n",
      " ---> ea73677a56de\n",
      "Successfully built ea73677a56de\n",
      "Successfully tagged wafer-data-processing:latest\n",
      "~/SageMaker/defect_detection/notebooks\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "pushd WM-811K/src/data_processing\n",
    "docker build -t  wafer-data-processing .\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "ecr_repository = 'wafer-data-processing'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "dask_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'wafer-data-processing' already exists in the registry with id '160951647621'\n",
      "The push refers to repository [160951647621.dkr.ecr.us-east-1.amazonaws.com/wafer-data-processing]\n",
      "\n",
      "\u001b[1Be3d65572: Preparing \n",
      "\u001b[1B8882d34b: Preparing \n",
      "\u001b[1Be8fd58aa: Preparing \n",
      "\u001b[1B60aea3a7: Preparing \n",
      "\u001b[1B19ebf025: Preparing \n",
      "\u001b[1Bb5ea98e6: Preparing \n",
      "\u001b[1Bdf52a3f7: Preparing \n",
      "\u001b[1B1063c032: Preparing \n",
      "\u001b[1B95b5fb80: Preparing \n",
      "\u001b[1B25f60f51: Preparing \n",
      "\u001b[1Bb9b0dbf8: Preparing \n",
      "\u001b[1B533a7525: Preparing \n",
      "\u001b[1B6f4e0e2d: Preparing \n",
      "\u001b[1Bbea216ea: Preparing \n",
      "\u001b[1B4da79a36: Preparing \n",
      "\u001b[1Bebd8fc35: Preparing \n",
      "\u001b[1Bcb249b79: Preparing \n",
      "\u001b[2Bcb249b79: Waiting g \n",
      "\u001b[1B4bce66cd: Layer already exists \u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:375870f763f68e8876ea6e61fc9873cd25cb755d174f977d99f41e2ac5b4a4f6 size: 4309\n"
     ]
    }
   ],
   "source": [
    "# Create ECR repository and push docker image\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $dask_repository_uri\n",
    "!docker push $dask_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://gianpo-public/sagemaker-2.9.2.dev0.tar.gz to ./sagemaker-2.9.2.dev0.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sys\n",
    "# import IPython\n",
    "# dist_version = '2.9.2.dev0'\n",
    "# !aws s3 cp s3://gianpo-public/sagemaker-{dist_version}.tar.gz .\n",
    "# !{sys.executable} -m pip install -q -U pip\n",
    "# !{sys.executable} -m pip install -q sagemaker-{dist_version}.tar.gz\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open(code_path / \"data_processing.py\", \"w\") as data_processing_script:\n",
    "    data_processing_script.write(\n",
    "        \"\"\"\n",
    "import sys\n",
    "import logging\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from dask.distributed import Client\n",
    "\n",
    "\n",
    "def hot_encode(img_arr):\n",
    "    new_arr = np.zeros((676, 3))\n",
    "    for x in range(676):\n",
    "        new_arr[x, img_arr[x]] = 1\n",
    "    return new_arr.reshape((26, 26, 3))\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    scheduler_ip = sys.argv[-1]\n",
    "    root_path = Path('/opt/ml/processing')\n",
    "    input_path = root_path / 'input'\n",
    "    output_path = root_path / 'train'\n",
    "    # Start the Dask cluster client\n",
    "    try:\n",
    "        client = Client(f\"tcp://{scheduler_ip}:8786\")\n",
    "        logging.info(f\"Printing cluster information: {client}\")\n",
    "    except Exception as err:\n",
    "        logging.exception(err)\n",
    "        \n",
    "    logging.info(\"Loading data\")\n",
    "    input_df = dd.from_pandas(\n",
    "        pd.read_pickle(\n",
    "            input_path / \"LSWMD.pkl\"\n",
    "        ).astype(\n",
    "            {\"waferIndex\": \"int32\"}\n",
    "        ),\n",
    "        npartitions=100)\n",
    "\n",
    "    logging.info(\"Cleaning data\")\n",
    "    clean_df = input_df.drop('waferIndex', axis=1)\n",
    "    clean_df['waferMapDim'] = clean_df.waferMap.apply(lambda x: x.shape, meta=pd.Series({'waferMapDim': [(0, 0)]}))\n",
    "    clean_df = clean_df[clean_df.waferMapDim.apply(lambda x: x[0] == x[1], meta=pd.Series({'x': True}))]\n",
    "    clean_df['label'] = clean_df.failureType.apply(lambda x: x[0, 0] if (isinstance(x, np.ndarray) and x.shape[0] > 0) else 'unknown', meta=pd.Series({\"x\": \"none\"}))\n",
    "    clean_df = clean_df[clean_df.label != \"unknown\"]\n",
    "    \n",
    "    label_dist = clean_df.groupby('label').size().compute()\n",
    "    inv_prob_label = {k: v for (k, v) in ((1/(label_dist/label_dist.sum()) )/((1/(label_dist/label_dist.sum()) ).sum())).iteritems()}\n",
    "    logging.info(f\"Labels: {inv_prob_label}\")\n",
    "    \n",
    "    x = np.stack(\n",
    "        clean_df.apply(\n",
    "            lambda x: x.waferMap.reshape(\n",
    "                (26, 26, 1)\n",
    "            ) if x.waferMapDim[0] == 26 else cv2.resize(\n",
    "                x.waferMap.reshape(x.waferMapDim[0], x.waferMapDim[1]), (26, 26)\n",
    "            ).reshape(26, 26, 1), axis=1, meta=pd.Series({'x': [np.zeros((26, 26, 1))]})).compute().values\n",
    "    )\n",
    "    x = np.apply_along_axis(hot_encode, axis=1, arr=x.reshape(-1, 26 * 26))\n",
    "    y = clean_df.label.compute().values\n",
    "    label_dist = clean_df.groupby('label').size().compute()\n",
    "    label_classes = numpy.array({l: i for (i, l) in enumerate(label_dist.index.values)})\n",
    "\n",
    "    logging.info(f\"x: {x.shape}\")\n",
    "    logging.info(f\"y: {y.shape}\")\n",
    "    np.savez_compressed(output_path / \"data.npz\", x=x, y=y, label_classes=label_classes)\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "dask_processor = ScriptProcessor(\n",
    "    base_job_name=\"wafer-data-processing\",\n",
    "    image_uri=dask_repository_uri,\n",
    "    command=[\"/opt/program/bootstrap.py\"],\n",
    "    role=role,\n",
    "    instance_count=4,\n",
    "    instance_type=\"local\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  wafer-data-processing-2020-10-01-23-28-07-823\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-10-01-23-28-07-823/input/input-1/LSWMD.pkl', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-10-01-23-28-07-823/input/code/data_processing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'autoencoder/train', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-10-01-23-28-07-823/output/autoencoder/train', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}]\n",
      "Creating ug67ffm7bs-algo-3-utsg6 ... \n",
      "Creating lnr438wdqp-algo-1-utsg6 ... \n",
      "Creating 92ze1gymvx-algo-4-utsg6 ... \n",
      "Creating 24liiyrcij-algo-2-utsg6 ... \n",
      "\u001b[2BAttaching to lnr438wdqp-algo-1-utsg6, ug67ffm7bs-algo-3-utsg6, 24liiyrcij-algo-2-utsg6, 92ze1gymvx-algo-4-utsg6\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.nanny - INFO -         Start Nanny at: 'tcp://172.18.0.2:40169'\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - -----------------------------------------------\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.nanny - INFO -         Start Nanny at: 'tcp://172.18.0.4:44581'\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.nanny - INFO -         Start Nanny at: 'tcp://172.18.0.5:42757'\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.nanny - INFO -         Start Nanny at: 'tcp://172.18.0.3:44193'\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - -----------------------------------------------\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Clear task state\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO -   Scheduler at:     tcp://172.18.0.4:8786\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO -   dashboard at:                     :8787\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO -       Start worker at:     tcp://172.18.0.4:37791\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO -          Listening to:     tcp://172.18.0.4:37791\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO -          dashboard at:           172.18.0.4:39443\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO - Waiting to connect to:      tcp://172.18.0.4:8786\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO -               Threads:                          8\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO -                Memory:                   64.38 GB\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO -       Local Directory: /dask-worker-space/worker-btf_z6ul\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Register worker <Worker 'tcp://172.18.0.4:37791', name: tcp://172.18.0.4:37791, memory: 0, processing: 0>\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Starting worker compute stream, tcp://172.18.0.4:37791\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.core - INFO - Starting established connection\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO -         Registered to:      tcp://172.18.0.4:8786\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.core - INFO - Starting established connection\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO -       Start worker at:     tcp://172.18.0.5:34117\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO -          Listening to:     tcp://172.18.0.5:34117\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO -          dashboard at:           172.18.0.5:35229\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO - Waiting to connect to:      tcp://172.18.0.4:8786\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO -               Threads:                          8\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO -                Memory:                   64.38 GB\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO -       Local Directory: /dask-worker-space/worker-w9_56z6v\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Register worker <Worker 'tcp://172.18.0.5:34117', name: tcp://172.18.0.5:34117, memory: 0, processing: 0>\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Starting worker compute stream, tcp://172.18.0.5:34117\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.core - INFO - Starting established connection\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO -         Registered to:      tcp://172.18.0.4:8786\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.core - INFO - Starting established connection\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO -       Start worker at:     tcp://172.18.0.2:37213\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO -          Listening to:     tcp://172.18.0.2:37213\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO -          dashboard at:           172.18.0.2:42943\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO - Waiting to connect to:      tcp://172.18.0.4:8786\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO -               Threads:                          8\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO -                Memory:                   64.38 GB\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO -       Local Directory: /dask-worker-space/worker-reh_6tvh\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Register worker <Worker 'tcp://172.18.0.2:37213', name: tcp://172.18.0.2:37213, memory: 0, processing: 0>\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Starting worker compute stream, tcp://172.18.0.2:37213\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.core - INFO - Starting established connection\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO -         Registered to:      tcp://172.18.0.4:8786\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.core - INFO - Starting established connection\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO -       Start worker at:     tcp://172.18.0.3:37071\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO -          Listening to:     tcp://172.18.0.3:37071\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO -          dashboard at:           172.18.0.3:41195\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO - Waiting to connect to:      tcp://172.18.0.4:8786\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO -               Threads:                          8\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO -                Memory:                   64.38 GB\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO -       Local Directory: /dask-worker-space/worker-0ipznlt8\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Register worker <Worker 'tcp://172.18.0.3:37071', name: tcp://172.18.0.3:37071, memory: 0, processing: 0>\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Starting worker compute stream, tcp://172.18.0.3:37071\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.core - INFO - Starting established connection\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO -         Registered to:      tcp://172.18.0.4:8786\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO - -------------------------------------------------\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.core - INFO - Starting established connection\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Receive client connection: Client-0038a822-043e-11eb-802c-0242ac120004\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.core - INFO - Starting established connection\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m Traceback (most recent call last):\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m   File \"/opt/ml/processing/input/code/data_processing.py\", line 62, in <module>\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m     label_classes = numpy.array({l: i for (i, l) in enumerate(label_dist.index.values)})\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m NameError: name 'numpy' is not defined\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Remove client Client-0038a822-043e-11eb-802c-0242ac120004\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Remove client Client-0038a822-043e-11eb-802c-0242ac120004\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 |\u001b[0m distributed.scheduler - INFO - Close client connection: Client-0038a822-043e-11eb-802c-0242ac120004\n",
      "\u001b[36m24liiyrcij-algo-2-utsg6 |\u001b[0m distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\n",
      "\u001b[35mug67ffm7bs-algo-3-utsg6 |\u001b[0m distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\n",
      "\u001b[33m92ze1gymvx-algo-4-utsg6 |\u001b[0m distributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\n",
      "\u001b[32mlnr438wdqp-algo-1-utsg6 exited with code 0\n",
      "\u001b[0mStopping 24liiyrcij-algo-2-utsg6 ... \n",
      "Stopping 92ze1gymvx-algo-4-utsg6 ... \n",
      "Stopping ug67ffm7bs-algo-3-utsg6 ... \n",
      "\u001b[1BAborting on container exit...... \u001b[32mdone\u001b[0m\n",
      "===== Job Complete =====\n",
      "."
     ]
    }
   ],
   "source": [
    "dask_processor.run(\n",
    "    code=str(code_path / 'data_processing.py'),\n",
    "    inputs=[ProcessingInput(\n",
    "        source=str(data_path / \"LSWMD.pkl\"),\n",
    "        destination='/opt/ml/processing/input'\n",
    "    )],\n",
    "    outputs=[ProcessingOutput(output_name='autoencoder/train', source='/opt/ml/processing/train')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "processed_data = dask_processor.latest_job.describe()['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bucket, *path = processed_data.split(\"/\")[2:]\n",
    "path = \"/\".join(path)\n",
    "print(bucket, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sagemaker.utils.download_file(bucket, path + \"/data.npz\", \"/tmp/data.npz\", sagemaker.session.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with np.load(\"/tmp/data.npz\", allow_pickle=True) as data:\n",
    "    x = data['arr_0']\n",
    "    y = data['arr_1']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
