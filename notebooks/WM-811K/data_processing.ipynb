{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#import dask.dataframe as dd\n",
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import cv2\n",
    "import os\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "root_path = Path('/home/ec2-user/SageMaker/defect_detection/')#.resolve()\n",
    "\n",
    "code_path = root_path / \"notebooks/WM-811K/src/\"\n",
    "code_path.mkdir(exist_ok=True)\n",
    "data_path = root_path / \"data/MIR-WM811K/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Creating SageMaker Processing Job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Build a Container for Dask Processing\n",
    "\n",
    "Create a container for processing with Dask. The code below is based on [this example](https://github.com/aws/amazon-sagemaker-examples/blob/master/sagemaker_processing/feature_transformation_with_sagemaker_processing_dask/feature_transformation_with_sagemaker_processing_dask.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~/SageMaker/defect_detection/notebooks/WM-811K/src/container ~/SageMaker/defect_detection/notebooks\n",
      "Sending build context to Docker daemon  16.38kB\n",
      "Step 1/21 : FROM continuumio/miniconda3:4.7.12\n",
      " ---> 406f2b43ea59\n",
      "Step 2/21 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> bee7c1789cd8\n",
      "Step 3/21 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil ffmpeg libsm6 libxext6\n",
      " ---> Using cache\n",
      " ---> ca8821c9b19a\n",
      "Step 4/21 : RUN pip3 install py4j psutil==5.6.5 numpy==1.17.4\n",
      " ---> Using cache\n",
      " ---> 44a6c1b8958e\n",
      "Step 5/21 : RUN apt-get clean\n",
      " ---> Using cache\n",
      " ---> 0b6d3e8f0d43\n",
      "Step 6/21 : RUN rm -rf /var/lib/apt/lists/*\n",
      " ---> Using cache\n",
      " ---> 1de28fac9d35\n",
      "Step 7/21 : ENV PYTHONHASHSEED 0\n",
      " ---> Using cache\n",
      " ---> 9119ae001b0b\n",
      "Step 8/21 : ENV PYTHONIOENCODING UTF-8\n",
      " ---> Using cache\n",
      " ---> d8659b8f85c9\n",
      "Step 9/21 : ENV PIP_DISABLE_PIP_VERSION_CHECK 1\n",
      " ---> Using cache\n",
      " ---> b5ec0227bc57\n",
      "Step 10/21 : RUN conda install --yes     -c conda-forge     python==3.8     python-blosc     cytoolz     dask==2.16.0     distributed==2.16.0     lz4     opencv     nomkl     numpy==1.18.1     pandas==1.0.5     tini==0.18.0     && conda clean -tipsy     && find /opt/conda/ -type f,l -name '*.a' -delete     && find /opt/conda/ -type f,l -name '*.pyc' -delete     && find /opt/conda/ -type f,l -name '*.js.map' -delete     && find /opt/conda/lib/python*/site-packages/bokeh/server/static -type f,l -name '*.js' -not -name '*.min.js' -delete     && rm -rf /opt/conda/pkgs\n",
      " ---> Using cache\n",
      " ---> cc9323fc7102\n",
      "Step 11/21 : RUN pip install dask-ml\n",
      " ---> Using cache\n",
      " ---> 6bce6338d2e6\n",
      "Step 12/21 : RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.0/dumb-init_1.2.0_amd64\n",
      " ---> Using cache\n",
      " ---> e734c072da14\n",
      "Step 13/21 : RUN chmod +x /usr/local/bin/dumb-init\n",
      " ---> Using cache\n",
      " ---> 33720a25ad10\n",
      "Step 14/21 : RUN apt-get update\n",
      " ---> Using cache\n",
      " ---> c1ddb9c3c2a1\n",
      "Step 15/21 : RUN apt-get install -y curl unzip python3 python3-setuptools python3-pip python-dev python3-dev python-psutil\n",
      " ---> Using cache\n",
      " ---> f2bca6f0af4a\n",
      "Step 16/21 : RUN conda install --yes s3fs -c conda-forge\n",
      " ---> Using cache\n",
      " ---> 45c8823afe93\n",
      "Step 17/21 : RUN mkdir /opt/app /etc/dask\n",
      " ---> Using cache\n",
      " ---> 04d67b588185\n",
      "Step 18/21 : COPY dask_config/dask.yaml /etc/dask/\n",
      " ---> Using cache\n",
      " ---> a326f698422c\n",
      "Step 19/21 : COPY program /opt/program\n",
      " ---> Using cache\n",
      " ---> f2f61e34e2fa\n",
      "Step 20/21 : RUN chmod +x /opt/program/bootstrap.py\n",
      " ---> Using cache\n",
      " ---> 0f21f71bda59\n",
      "Step 21/21 : ENTRYPOINT [\"/opt/program/bootstrap.py\"]\n",
      " ---> Using cache\n",
      " ---> ea73677a56de\n",
      "Successfully built ea73677a56de\n",
      "Successfully tagged wafer-data-processing:latest\n",
      "~/SageMaker/defect_detection/notebooks\n"
     ]
    }
   ],
   "source": [
    "%%sh \n",
    "pushd WM-811K/src/container\n",
    "docker build -t  wafer-data-processing .\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Using --password via the CLI is insecure. Use --password-stdin.\n",
      "WARNING! Your password will be stored unencrypted in /home/ec2-user/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n",
      "\n",
      "An error occurred (RepositoryAlreadyExistsException) when calling the CreateRepository operation: The repository with name 'wafer-data-processing' already exists in the registry with id '160951647621'\n",
      "The push refers to repository [160951647621.dkr.ecr.us-east-1.amazonaws.com/wafer-data-processing]\n",
      "\n",
      "\u001b[1Be3d65572: Preparing \n",
      "\u001b[1B8882d34b: Preparing \n",
      "\u001b[1Be8fd58aa: Preparing \n",
      "\u001b[1B60aea3a7: Preparing \n",
      "\u001b[1B19ebf025: Preparing \n",
      "\u001b[1Bb5ea98e6: Preparing \n",
      "\u001b[1Bdf52a3f7: Preparing \n",
      "\u001b[1B1063c032: Preparing \n",
      "\u001b[1B95b5fb80: Preparing \n",
      "\u001b[1B25f60f51: Preparing \n",
      "\u001b[1Bb9b0dbf8: Preparing \n",
      "\u001b[1B533a7525: Preparing \n",
      "\u001b[1B6f4e0e2d: Preparing \n",
      "\u001b[1Bbea216ea: Preparing \n",
      "\u001b[1B4da79a36: Preparing \n",
      "\u001b[1Bebd8fc35: Preparing \n",
      "\u001b[1Bcb249b79: Preparing \n",
      "\u001b[2Bcb249b79: Waiting g \n",
      "\u001b[1B4bce66cd: Layer already exists \u001b[13A\u001b[2K\u001b[9A\u001b[2K\u001b[2A\u001b[2Klatest: digest: sha256:375870f763f68e8876ea6e61fc9873cd25cb755d174f977d99f41e2ac5b4a4f6 size: 4309\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "account_id = boto3.client('sts').get_caller_identity().get('Account')\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "ecr_repository = 'wafer-data-processing'\n",
    "tag = ':latest'\n",
    "uri_suffix = 'amazonaws.com'\n",
    "dask_repository_uri = '{}.dkr.ecr.{}.{}/{}'.format(account_id, region, uri_suffix, ecr_repository + tag)\n",
    "\n",
    "# Create ECR repository and push docker image\n",
    "!$(aws ecr get-login --region $region --registry-ids $account_id --no-include-email)\n",
    "!aws ecr create-repository --repository-name $ecr_repository\n",
    "!docker tag {ecr_repository + tag} $dask_repository_uri\n",
    "!docker push $dask_repository_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://gianpo-public/sagemaker-2.9.2.dev0.tar.gz to ./sagemaker-2.9.2.dev0.tar.gz\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import IPython\n",
    "dist_version = '2.9.2.dev0'\n",
    "!aws s3 cp s3://gianpo-public/sagemaker-{dist_version}.tar.gz .\n",
    "!{sys.executable} -m pip install -q -U pip\n",
    "!{sys.executable} -m pip install -q sagemaker-{dist_version}.tar.gz\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with open(code_path / \"data_processing.py\", \"w\") as data_processing_script:\n",
    "    data_processing_script.write(\n",
    "        \"\"\"\n",
    "import sys\n",
    "import logging\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from dask.distributed import Client\n",
    "\n",
    "\n",
    "def hot_encode(img_arr):\n",
    "    new_arr = np.zeros((676, 3))\n",
    "    for x in range(676):\n",
    "        new_arr[x, img_arr[x]] = 1\n",
    "    return new_arr.reshape((26, 26, 3))\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    scheduler_ip = sys.argv[-1]\n",
    "    root_path = Path('/opt/ml/processing')\n",
    "    input_path = root_path / 'input'\n",
    "    output_path = root_path / 'train'\n",
    "    # Start the Dask cluster client\n",
    "    try:\n",
    "        client = Client(f\"tcp://{scheduler_ip}:8786\")\n",
    "        logging.info(f\"Printing cluster information: {client}\")\n",
    "    except Exception as err:\n",
    "        logging.exception(err)\n",
    "        \n",
    "    logging.info(\"Loading data\")\n",
    "    input_df = dd.from_pandas(\n",
    "        pd.read_pickle(\n",
    "            input_path / \"LSWMD.pkl\"\n",
    "        ).astype(\n",
    "            {\"waferIndex\": \"int32\"}\n",
    "        ),\n",
    "        npartitions=100)\n",
    "\n",
    "    logging.info(\"Cleaning data\")\n",
    "    clean_df = input_df.drop('waferIndex', axis=1)\n",
    "    clean_df['waferMapDim'] = clean_df.waferMap.apply(lambda x: x.shape, meta=pd.Series({'waferMapDim': [(0, 0)]}))\n",
    "    clean_df = clean_df[clean_df.waferMapDim.apply(lambda x: x[0] == x[1], meta=pd.Series({'x': True}))]\n",
    "    clean_df['label'] = clean_df.failureType.apply(lambda x: x[0, 0] if (isinstance(x, np.ndarray) and x.shape[0] > 0) else 'unknown', meta=pd.Series({\"x\": \"none\"}))\n",
    "    clean_df = clean_df[clean_df.label != \"unknown\"]\n",
    "    \n",
    "    label_dist = clean_df.groupby('label').size().compute()\n",
    "    inv_prob_label = {k: v for (k, v) in ((1/(label_dist/label_dist.sum()) )/((1/(label_dist/label_dist.sum()) ).sum())).iteritems()}\n",
    "    logging.info(f\"Labels: {inv_prob_label}\")\n",
    "    \n",
    "    x = np.stack(\n",
    "        clean_df.apply(\n",
    "            lambda x: x.waferMap.reshape(\n",
    "                (26, 26, 1)\n",
    "            ) if x.waferMapDim[0] == 26 else cv2.resize(\n",
    "                x.waferMap.reshape(x.waferMapDim[0], x.waferMapDim[1]), (26, 26)\n",
    "            ).reshape(26, 26, 1), axis=1, meta=pd.Series({'x': [np.zeros((26, 26, 1))]})).compute().values\n",
    "    )\n",
    "    x = np.apply_along_axis(hot_encode, axis=1, arr=x.reshape(-1, 26 * 26))\n",
    "    y = clean_df.label.compute().values\n",
    "\n",
    "    logging.info(f\"x: {x.shape}\")\n",
    "    logging.info(f\"y: {y.shape}\")\n",
    "    np.savez_compressed(output_path / \"data.npz\", x, y)\n",
    "    \"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "\n",
    "region = boto3.session.Session().region_name\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "dask_processor = ScriptProcessor(\n",
    "    base_job_name=\"wafer-data-processing\",\n",
    "    image_uri=dask_repository_uri,\n",
    "    command=[\"/opt/program/bootstrap.py\"],\n",
    "    role=role,\n",
    "    instance_count=4,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  wafer-data-processing-2020-09-30-22-55-17-588\n",
      "Inputs:  [{'InputName': 'input-1', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-09-30-22-55-17-588/input/input-1/LSWMD.pkl', 'LocalPath': '/opt/ml/processing/input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-09-30-22-55-17-588/input/code/data_processing.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'train_data', 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-160951647621/wafer-data-processing-2020-09-30-22-55-17-588/output/train_data', 'LocalPath': '/opt/ml/processing/train', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...........................\u001b[34mdistributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.165.171:45765'\u001b[0m\n",
      "\u001b[34mdistributed.http.proxy - INFO - To route to workers diagnostics web server please install jupyter-server-proxy: python -m pip install jupyter-server-proxy\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - -----------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Clear task state\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO -   Scheduler at:   tcp://10.0.165.171:8786\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO -   dashboard at:                     :8787\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -       Start worker at:   tcp://10.0.165.171:36765\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -          Listening to:   tcp://10.0.165.171:36765\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -          dashboard at:         10.0.165.171:38101\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.165.171:8786\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -               Threads:                          4\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -                Memory:                   14.98 GB\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -       Local Directory: /dask-worker-space/worker-ft30ksck\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.165.171:36765', name: tcp://10.0.165.171:36765, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.165.171:36765\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO -         Registered to:    tcp://10.0.165.171:8786\u001b[0m\n",
      "\u001b[34mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.142.20:32847', name: tcp://10.0.142.20:32847, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.142.20:32847\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.173.31:44587', name: tcp://10.0.173.31:44587, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.173.31:44587\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Register worker <Worker 'tcp://10.0.162.189:33271', name: tcp://10.0.162.189:33271, memory: 0, processing: 0>\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Starting worker compute stream, tcp://10.0.162.189:33271\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[32mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.142.20:34615'\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -       Start worker at:    tcp://10.0.142.20:32847\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -          Listening to:    tcp://10.0.142.20:32847\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -          dashboard at:          10.0.142.20:38617\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.165.171:8786\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -               Threads:                          4\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -                Memory:                   15.16 GB\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -       Local Directory: /dask-worker-space/worker-n_m9pqt9\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO -         Registered to:    tcp://10.0.165.171:8786\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[32mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[36mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.173.31:39075'\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -       Start worker at:    tcp://10.0.173.31:44587\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -          Listening to:    tcp://10.0.173.31:44587\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -          dashboard at:          10.0.173.31:33767\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.165.171:8786\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -               Threads:                          4\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -                Memory:                   14.95 GB\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -       Local Directory: /dask-worker-space/worker-bgn_9jf_\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO -         Registered to:    tcp://10.0.165.171:8786\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[36mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[35mdistributed.nanny - INFO -         Start Nanny at: 'tcp://10.0.162.189:46639'\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -       Start worker at:   tcp://10.0.162.189:33271\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -          Listening to:   tcp://10.0.162.189:33271\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -          dashboard at:         10.0.162.189:43659\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - Waiting to connect to:    tcp://10.0.165.171:8786\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -               Threads:                          4\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -                Memory:                   14.98 GB\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -       Local Directory: /dask-worker-space/worker-8no2zqy8\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO -         Registered to:    tcp://10.0.165.171:8786\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - -------------------------------------------------\u001b[0m\n",
      "\u001b[35mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Receive client connection: Client-b4f5eb1f-0370-11eb-8020-525e7713473e\u001b[0m\n",
      "\u001b[34mdistributed.core - INFO - Starting established connection\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.8/site-packages/dask/dataframe/core.py:3095: UserWarning: \u001b[0m\n",
      "\u001b[34mYou did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\u001b[0m\n",
      "\u001b[34mLoading data\u001b[0m\n",
      "\u001b[34mCleaning data\u001b[0m\n",
      "\u001b[34mLabels: {'Center': 0.010594450500847985, 'Donut': 0.6076674108700665, 'Edge-Loc': 0.01251079963556019, 'Edge-Ring': 0.0077834801026358015, 'Loc': 0.013418523268424182, 'Near-full': 0.2658544922556541, 'Random': 0.030167885646031668, 'Scratch': 0.051559659104126854, 'none': 0.0004432986166526461}\u001b[0m\n",
      "\u001b[34mx: (22894, 26, 26, 3)\u001b[0m\n",
      "\u001b[34my: (22894,)\u001b[0m\n",
      "\u001b[34mTo provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n",
      "  Before: .apply(func)\n",
      "  After:  .apply(func, meta=('waferMapDim', 'bool'))\n",
      "\n",
      "  warnings.warn(meta_warning(meta))\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Remove client Client-b4f5eb1f-0370-11eb-8020-525e7713473e\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Remove client Client-b4f5eb1f-0370-11eb-8020-525e7713473e\u001b[0m\n",
      "\u001b[34mdistributed.scheduler - INFO - Close client connection: Client-b4f5eb1f-0370-11eb-8020-525e7713473e\u001b[0m\n",
      "\u001b[35mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[35mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[32mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[32mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\u001b[36mdistributed.worker - INFO - Connection to scheduler broken.  Reconnecting...\u001b[0m\n",
      "\u001b[36mReceived a shutdown signal from Dask cluster\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dask_processor.run(\n",
    "    code=str(code_path / 'data_processing.py'),\n",
    "    inputs=[ProcessingInput(\n",
    "        source=str(data_path / \"LSWMD.pkl\"),\n",
    "        destination='/opt/ml/processing/input'\n",
    "    )],\n",
    "    outputs=[ProcessingOutput(output_name='train_data', source='/opt/ml/processing/train')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "processed_data = dask_processor.latest_job.describe()['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "bucket, *path = processed_data.split(\"/\")[2:]\n",
    "path = \"/\".join(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "sagemaker.utils.download_file(bucket, path + \"/data.npz\", \"/tmp/data.npz\", sagemaker.session.Session())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "with np.load(\"/tmp/data.npz\", allow_pickle=True) as data:\n",
    "    x = data['arr_0']\n",
    "    y = data['arr_1']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
