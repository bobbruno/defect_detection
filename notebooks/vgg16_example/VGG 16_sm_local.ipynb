{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification Neo Compilation Example - Local Mode\n",
    "\n",
    "This notebook shows an intermediate step in the process of developing an Edge image classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "https://github.com/matplotlib/matplotlib/blob/v3.1.3/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/__init__.py:1473: The name tf.estimator.inputs is deprecated. Please use tf.compat.v1.estimator.inputs instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.15.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import os\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions and scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(img_path):\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img.astype(float), (224, 224)) #resize\n",
    "    imgMean = np.array([104, 117, 124], np.float)\n",
    "    img -= imgMean #subtract image mean\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_encode(class_id):\n",
    "    enc = np.zeros((257),dtype=int)\n",
    "    enc[class_id] = 1\n",
    "    return enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_graph(model_dir, output_node_names):\n",
    "    \"\"\"Extract the sub graph defined by the output nodes and convert \n",
    "    all its variables into constant \n",
    "    Args:\n",
    "        model_dir: the root folder containing the checkpoint state file\n",
    "        output_node_names: a string, containing all the output node's names, \n",
    "                            comma separated\n",
    "    \"\"\"\n",
    "    if not tf.gfile.Exists(model_dir):\n",
    "        raise AssertionError(\n",
    "            \"Export directory doesn't exists. Please specify an export \"\n",
    "            \"directory: %s\" % model_dir)\n",
    "\n",
    "    if not output_node_names:\n",
    "        print(\"You need to supply the name of a node to --output_node_names.\")\n",
    "        return -1\n",
    "\n",
    "    # We retrieve our checkpoint fullpath\n",
    "    checkpoint = tf.train.get_checkpoint_state(model_dir)\n",
    "    input_checkpoint = checkpoint.model_checkpoint_path\n",
    "    \n",
    "    # We precise the file fullname of our freezed graph\n",
    "    absolute_model_dir = \"/\".join(input_checkpoint.split('/')[:-1])\n",
    "    output_graph = absolute_model_dir + \"/frozen_model.pb\"\n",
    "\n",
    "    # We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "    clear_devices = True\n",
    "\n",
    "    # We start a session using a temporary fresh Graph\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        # We import the meta graph in the current default Graph\n",
    "        saver = tf.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "        # We restore the weights\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "\n",
    "        # We use a built-in TF helper to export variables to constants\n",
    "        output_graph_def = tf.graph_util.convert_variables_to_constants(\n",
    "            sess, # The session is used to retrieve the weights\n",
    "            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "            output_node_names.split(\",\") # The output node names are used to select the usefull nodes\n",
    "        ) \n",
    "\n",
    "        # Finally we serialize and dump the output graph to the filesystem\n",
    "        with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
    "\n",
    "    return output_graph_def"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class VGG(object):\n",
    "    \"\"\"alexNet model\"\"\"\n",
    "    def __init__(self, n_classes, batch_size=None):\n",
    "        self.NUM_CLASSES = n_classes\n",
    "        self.BATCH_SIZE = batch_size\n",
    "        self.x = tf.placeholder(tf.float32, [None, 224, 224, 3])\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.NUM_CLASSES])\n",
    "        self.buildCNN()\n",
    "\n",
    "    def buildCNN(self):\n",
    "        \"\"\"build model\"\"\"\n",
    "        \n",
    "        input_layer = self.x\n",
    "        \n",
    "        conv1_1 = tf.layers.conv2d(\n",
    "            input_layer, filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv1_1'\n",
    "        )\n",
    "\n",
    "        conv1_2 = tf.layers.conv2d(\n",
    "            conv1_1, filters=64, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv1_2'\n",
    "        )\n",
    "        pool1 = tf.layers.max_pooling2d(conv1_2, pool_size=(2, 2), strides=(2, 2), padding='SAME', name='pool1')\n",
    "\n",
    "        # conv2\n",
    "        conv2_1 = tf.layers.conv2d(\n",
    "            pool1, filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv2_1'\n",
    "        )\n",
    "        conv2_2 = tf.layers.conv2d(\n",
    "            conv2_1, filters=128, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv2_2'\n",
    "        )\n",
    "        pool2 = tf.layers.max_pooling2d(conv2_2, pool_size=(2, 2), strides=(2, 2), padding='SAME', name='pool2')\n",
    "\n",
    "        # conv3\n",
    "        conv3_1 = tf.layers.conv2d(\n",
    "            pool2, filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv3_1'\n",
    "        )\n",
    "        conv3_2 = tf.layers.conv2d(\n",
    "            conv3_1, filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv3_2'\n",
    "        )\n",
    "        conv3_3 = tf.layers.conv2d(\n",
    "            conv3_2, filters=256, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv3_3'\n",
    "        )\n",
    "        pool3 = tf.layers.max_pooling2d(conv3_3, pool_size=(2, 2), strides=(2, 2), padding='SAME', name='pool3')\n",
    "\n",
    "        # conv4\n",
    "        conv4_1 = tf.layers.conv2d(\n",
    "            pool3, filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv4_1'\n",
    "        )\n",
    "        conv4_2 = tf.layers.conv2d(\n",
    "            conv4_1, filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv4_2'\n",
    "        )\n",
    "        conv4_3 = tf.layers.conv2d(\n",
    "            conv4_2, filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv4_3'\n",
    "        )\n",
    "        pool4 = tf.layers.max_pooling2d(conv4_3, pool_size=(2, 2), strides=(2, 2), padding='SAME', name='pool4')\n",
    "\n",
    "        # conv5\n",
    "        conv5_1 = tf.layers.conv2d(\n",
    "            pool4, filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv5_1'\n",
    "        )\n",
    "        conv5_2 = tf.layers.conv2d(\n",
    "            conv5_1, filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv5_2'\n",
    "        )\n",
    "        conv5_3 = tf.layers.conv2d(\n",
    "            conv5_2, filters=512, kernel_size=(3, 3), strides=(1, 1), activation=tf.nn.relu, padding='SAME', name='conv5_3'\n",
    "        )\n",
    "        pool5 = tf.layers.max_pooling2d(conv5_3, pool_size=(2, 2), strides=(2, 2), padding='SAME', name='pool5')\n",
    "        #print('POOL5', pool5.shape)\n",
    "        #CULPADO flatten = tf.layers.flatten(pool5, name='flatten')\n",
    "        flatten = tf.reshape(pool5, [-1, 7*7*512])\n",
    "        fc1_relu = tf.layers.dense(flatten, units=4096, activation=tf.nn.relu, name='fc1_relu')\n",
    "        fc2_relu = tf.layers.dense(fc1_relu, units=4096, activation=tf.nn.relu, name='fc2_relu')\n",
    "        self.logits = tf.layers.dense(fc2_relu, units=self.NUM_CLASSES, name='fc3_relu')\n",
    "        #fc3 = tf.nn.softmax(logits)\n",
    "        print(self.logits.shape)\n",
    "        # Return the complete AlexNet model\n",
    "        return self.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-5-17995d801f3a>:16: conv2d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.keras.layers.Conv2D` instead.\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/layers/convolutional.py:424: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-17995d801f3a>:22: max_pooling2d (from tensorflow.python.layers.pooling) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.MaxPooling2D instead.\n",
      "WARNING:tensorflow:From <ipython-input-5-17995d801f3a>:71: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "(?, 257)\n"
     ]
    }
   ],
   "source": [
    "net = VGG(257)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Placeholder:0' shape=(?, 224, 224, 3) dtype=float32>,\n",
       " <tf.Tensor 'Placeholder_1:0' shape=(?, 257) dtype=float32>,\n",
       " <tf.Tensor 'fc3_relu/BiasAdd:0' shape=(?, 257) dtype=float32>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.x, net.y, net.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example is no longer on this URL. Now it has to be downloaded from Google Drive: https://drive.google.com/uc?id=1r6o0pSROcV1_VwT4oSjA2FBUSCWGuxLK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y gdown\n",
    "!mkdir -p ../data/Caltech256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "url = \"https://drive.google.com/uc?id=1r6o0pSROcV1_VwT4oSjA2FBUSCWGuxLK\"\n",
    "destination = \"../data/Caltech256/256_ObjectCategories.tar\"\n",
    "gdown.download(url, destination, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!curl http://www.vision.caltech.edu/Image_Datasets/Caltech256/256_ObjectCategories.tar -o ../data/Caltech256/256_ObjectCategories.tar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "pushd ../data/Caltech256/\n",
    "tar -xvf 256_ObjectCategories.tar\n",
    "popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "counter = 0\n",
    "classes = {}\n",
    "all_image_paths = list(glob.glob('../data/Caltech256/256_ObjectCategories/*/*.jpg'))\n",
    "x = []\n",
    "y = []\n",
    "for i in all_image_paths:\n",
    "    _,cat,fname = i.split('/')[3:]\n",
    "    if classes.get(cat) is None: \n",
    "        classes[cat] = counter\n",
    "        counter += 1\n",
    "\n",
    "    x.append(i)\n",
    "    y.append(classes.get(cat))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (6) Define model's cost and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=net.logits, labels=net.y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# (7) Defining evaluation metrics\n",
    "correct_prediction = tf.equal(tf.argmax(net.logits, 1), tf.argmax(net.y, 1))\n",
    "accuracy_pct = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) * 100\n",
    "\n",
    "# (8) initialize\n",
    "initializer_op = tf.global_variables_initializer()\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 128\n",
    "test_batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 1 epochs.\n",
      "Batch 48/160: batch_cost=5.529, batch_acc=3.906,avg_cost=14.893,avg_acc=0.869\n",
      "Batch 98/160: batch_cost=5.520, batch_acc=1.562,avg_cost=16.603,avg_acc=1.914\n",
      "Batch 148/160: batch_cost=5.458, batch_acc=3.125,avg_cost=18.291,avg_acc=3.018\n",
      "Epoch 1: Training Cost = 18.661, Training Acc = 3.33 -- Test Cost = 5.251, Test Acc = 5.86\n",
      "\n",
      "\n",
      "Training Completed. Final Evaluation on Test Data Set.\n",
      "\n",
      "Test Cost: 5.251\n",
      "Test Accuracy: 5.86%\n",
      "\n",
      "\n",
      "Evaluation on Validation Data Set.\n",
      "\n",
      "Evaluation Cost: 5.319\n",
      "Evaluation Accuracy: 4.30%\n"
     ]
    }
   ],
   "source": [
    "# (9) Run\n",
    "with tf.Session() as session:\n",
    "    session.run(initializer_op)\n",
    "    \n",
    "    print(\"Training for\", epochs, \"epochs.\")\n",
    "    \n",
    "    # looping over epochs: \n",
    "    for epoch in range(epochs):\n",
    "        # To monitor performance during training\n",
    "        avg_cost = 0.0 \n",
    "        avg_acc_pct = 0.0\n",
    "        \n",
    "        # loop over all batches of the epoch- 1088 records  \n",
    "        # batch_size = 128 is already defined\n",
    "        n_batches = int(len(x_train) / batch_size)\n",
    "        counter = 1\n",
    "        for i in range(n_batches):\n",
    "            # Get the random int for batch\n",
    "            #random_indices = np.random.randint(len(x_train), size=batch_size) # 1088 is the no of training set records\n",
    "            pivot = i * batch_size\n",
    "            feed = {\n",
    "                net.x: [get_img(i) for i in x_train[pivot:pivot+batch_size]],\n",
    "                net.y: [get_label_encode(i) for i in y_train[pivot:pivot+batch_size]]\n",
    "            }\n",
    "            \n",
    "            # feed batch data to run optimization and fetching cost and accuracy: \n",
    "            _, batch_cost, batch_acc = session.run([optimizer, cost, accuracy_pct], \n",
    "                                                   feed_dict=feed)\n",
    "            # Print batch cost to see the code is working (optional)\n",
    "            # print('Batch no. {}: batch_cost: {}, batch_acc: {}'.format(counter, batch_cost, batch_acc))\n",
    "            # Get the average cost and accuracy for all batches: \n",
    "            avg_cost += batch_cost / n_batches\n",
    "            avg_acc_pct += batch_acc / n_batches\n",
    "            counter += 1\n",
    "            if counter % 50 == 0:\n",
    "                print(\"Batch {}/{}: batch_cost={:.3f}, batch_acc={:.3f},avg_cost={:.3f},avg_acc={:.3f}\".format(\n",
    "                    i,n_batches, batch_cost, batch_acc, avg_cost, avg_acc_pct\n",
    "                ))\n",
    "        \n",
    "        # Get cost and accuracy after one iteration\n",
    "        test_cost = cost.eval({net.x: [get_img(i) for i in x_test[:test_batch_size]], net.y: [get_label_encode(i) for i in y_test[:test_batch_size]]})\n",
    "        test_acc_pct = accuracy_pct.eval({net.x: [get_img(i) for i in x_test[:test_batch_size]], net.y: [get_label_encode(i) for i in y_test[:test_batch_size]]})\n",
    "        # output logs at end of each epoch of training:\n",
    "        print(\"Epoch {}: Training Cost = {:.3f}, Training Acc = {:.2f} -- Test Cost = {:.3f}, Test Acc = {:.2f}\"\\\n",
    "              .format(epoch + 1, avg_cost, avg_acc_pct, test_cost, test_acc_pct))\n",
    "\n",
    "    # Getting Final Test Evaluation\n",
    "    print('\\n')\n",
    "    print(\"Training Completed. Final Evaluation on Test Data Set.\\n\") \n",
    "    test_cost = cost.eval({net.x: [get_img(i) for i in x_test[:test_batch_size]], net.y: [get_label_encode(i) for i in y_test[:test_batch_size]]})\n",
    "    test_accy_pct = accuracy_pct.eval({net.x: [get_img(i) for i in x_test[:test_batch_size]], net.y: [get_label_encode(i) for i in y_test[:test_batch_size]]})\n",
    "    print(\"Test Cost:\", '{:.3f}'.format(test_cost))\n",
    "    print(\"Test Accuracy: \", '{:.2f}'.format(test_accy_pct), \"%\", sep='')\n",
    "    print('\\n')\n",
    "    \n",
    "    # Getting accuracy on Validation set  \n",
    "    val_cost = cost.eval({net.x: [get_img(i) for i in x_test[test_batch_size:test_batch_size*2]], net.y: [get_label_encode(i) for i in y_test[test_batch_size:test_batch_size*2]]})\n",
    "    val_acc_pct = accuracy_pct.eval({net.x: [get_img(i) for i in x_test[test_batch_size:test_batch_size*2]], net.y: [get_label_encode(i) for i in y_test[test_batch_size:test_batch_size*2]]})\n",
    "    print(\"Evaluation on Validation Data Set.\\n\")\n",
    "    print(\"Evaluation Cost:\", '{:.3f}'.format(val_cost))\n",
    "    print(\"Evaluation Accuracy: \", '{:.2f}'.format(val_acc_pct), \"%\", sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/exporting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 224, 224, 3)\n",
      "Elapsed time: 0.35367631912231445\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf vgg16/\n",
    "exporter = tf.saved_model.builder.SavedModelBuilder('vgg16/model')\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    img = get_img('TestImage.jpg')\n",
    "    batch = np.array([img for i in range(batch_size)]).reshape((batch_size,224,224,3))\n",
    "    print(batch.shape)\n",
    "    x = net.x\n",
    "    y = net.logits\n",
    "    start_time = time.time()\n",
    "    out = sess.run(y, feed_dict = {x: batch})\n",
    "    print(\"Elapsed time: {}\".format(time.time() - start_time))\n",
    "    \n",
    "    exporter.add_meta_graph_and_variables(\n",
    "        sess, \n",
    "        tags=[tf.saved_model.tag_constants.SERVING], \n",
    "        signature_def_map={\n",
    "            tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY:\n",
    "            tf.saved_model.signature_def_utils.predict_signature_def(\n",
    "                inputs={\"inputs\": net.x}, \n",
    "                outputs={\"outputs\": net.logits }\n",
    "            )\n",
    "        },\n",
    "        strip_default_attrs=True)\n",
    "    #exporter.save()\n",
    "    saver.save(sess, 'vgg16/model/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vgg16/model/model\n",
      "WARNING:tensorflow:From <ipython-input-4-da836ea2d438>:41: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/tensorflow_core/python/framework/graph_util_impl.py:277: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "INFO:tensorflow:Froze 32 variables.\n",
      "INFO:tensorflow:Converted 32 variables to const ops.\n",
      "119 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "_ = freeze_graph('vgg16/model', 'fc3_relu/BiasAdd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frozen_model.pb\n"
     ]
    }
   ],
   "source": [
    "!cd vgg16 && rm -f model.tar.gz && cd model && tar -czvf ../model.tar.gz frozen_model.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sagemaker\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "# Retrieve the default bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "default_bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sagemaker.session.Session"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sagemaker_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: vgg16/model.tar.gz to s3://sagemaker-us-east-1-160951647621/neo/VGG16/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "role=sagemaker.session.get_execution_role()\n",
    "job_prefix='VGG16'\n",
    "path='neo/%s' % job_prefix\n",
    "\n",
    "sm = boto3.client('sagemaker')\n",
    "!aws s3 cp vgg16/model.tar.gz s3://$default_bucket/$path/model.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_device='ml_m5'\n",
    "# 'lambda'|'ml_m4'|'ml_m5'|'ml_c4'|'ml_c5'|'ml_p2'|'ml_p3'|'ml_g4dn'|'ml_inf1'|'jetson_tx1'|'jetson_tx2'|'jetson_nano'|'jetson_xavier'|\n",
    "# 'rasp3b'|'imx8qm'|'deeplens'|'rk3399'|'rk3288'|'aisage'|'sbe_c'|'qcs605'|'qcs603'|'sitara_am57x'|'amba_cv22'|'x86_win32'|'x86_win64'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG16-1601381999\n"
     ]
    }
   ],
   "source": [
    "job_name=\"%s-%d\" % (job_prefix, int(time.time()))\n",
    "sm.create_compilation_job(\n",
    "    CompilationJobName=job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': \"s3://%s/%s/model.tar.gz\" % (default_bucket, path),\n",
    "        'DataInputConfig': '{\"data\":[1,224,224,3]}',\n",
    "        'Framework': 'TENSORFLOW'\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': \"s3://%s/%s/\" % (default_bucket, path),\n",
    "        'TargetDevice': target_device\n",
    "    },\n",
    "    StoppingCondition={\n",
    "        'MaxRuntimeInSeconds': 300\n",
    "    }\n",
    ")\n",
    "print(job_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sagemaker-us-east-1-160951647621'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-160951647621/neo/VGG16/model-ml_m5.tar.gz\n",
      "fatal error: An error occurred (404) when calling the HeadObject operation: Key \"neo/VGG16/model-ml_m5.tar.gz\" does not exist\n"
     ]
    }
   ],
   "source": [
    "!echo s3://$default_bucket/neo/VGG16/model-$target_device\\.tar.gz\n",
    "!aws s3 cp s3://$default_bucket/neo/VGG16/model-$target_device\\.tar.gz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -tzvf model-$target_device\\.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf neo_test && mkdir neo_test\n",
    "!tar -xzvf model-$target_device\\.tar.gz -C neo_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ldd neo_test/compiled.so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "from dlr import DLRModel\n",
    "\n",
    "model_path='neo_test'\n",
    "imgMean = np.array([104, 117, 124], np.float)\n",
    "img = cv2.imread(\"TestImage.jpg\")\n",
    "img = cv2.resize(img.astype(float), (224, 224)) # resize\n",
    "img -= imgMean #subtract image mean\n",
    "img = img.reshape((1, 224, 224, 3))\n",
    "\n",
    "device = 'cpu'                           # Go, Raspberry Pi, go!\n",
    "model = DLRModel(model_path, dev_type=device)\n",
    "\n",
    "print(model.get_input_names())\n",
    "\n",
    "def predict(img):\n",
    "    start = time.time()\n",
    "    input_data = {'Placeholder': img}\n",
    "    out = model.run(input_data)\n",
    "    return (out, time.time()-start)\n",
    "startTime = time.time()\n",
    "out = [predict(img)[1] for i in (range(1))]\n",
    "print(\"Elapsed time: {}\".format((time.time() - start_time)))\n",
    "#top1 = np.argmax(out[0])\n",
    "#prob = np.max(out)\n",
    "#print(\"Class: %d, probability: %f\" % (top1, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.plot(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
